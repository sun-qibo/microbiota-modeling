{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255b5470-b5df-4ee9-aa8b-c140dc107db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "difference from the notebooks of age pred:\n",
    "1. no pred \n",
    "2. use all the microbiota data instead of only those who filtered by age\n",
    "3. vis with different meta data factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da73769-c74f-46f8-a3c6-36397522a711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263045b6-ae3a-4c2b-aa94-ff4b4c2e15d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054eac6f-478d-40c0-aae5-96f3bfc128ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "can we cluster enterotypes by the latents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d615ce7-aa6a-481f-824c-d50764b9d164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3c0791-eec3-4363-a464-a9f052d2be00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0468817f-3744-4f1a-b1f6-286d822602a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_genus = pd.read_csv(\"../data/processed_genus_log_drop08_scaled.csv\", header=0, index_col=0, sep='\\t')\n",
    "df_genus = pd.read_csv(\"../data/genus_counts_log_scaled_reduced.csv\", header=0, index_col=0, sep='\\t')\n",
    "\n",
    "print(df_genus.shape)\n",
    "X = df_genus.to_numpy()\n",
    "input_dim = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0684ba88-95bd-4189-86e1-11f467d1c7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(range(X.shape[0]), test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train = X[train_idx, :]\n",
    "X_val = X[val_idx, :]\n",
    "X_test = X[test_idx, :]\n",
    "\n",
    "# Convert to tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, X_val_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "(X_train!=0).sum() / 0.7 / ((X_val!=0).sum() / 0.3)   # expect to be near 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bcc55c-994d-44e4-af09-f5a0c3a6c69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9f6793-9289-48f5-8292-c44fe10e38be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from models_attention_with_regressor_head import *\n",
    "\n",
    "# todo  add noise (e.g., zero out random values) to the input during training and train the network to reconstruct the original data. This encourages the model to learn robust features despite the sparse noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88781587-ddaf-4743-87c3-a734227fb504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce8e71e-ce4f-4d2e-9005-be6f0929836d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from loss_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ca5cbb-7832-43e7-9c34-a50596e656be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba8b507-0a2c-44cb-a59b-a76ffee077f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, model_name, train_loader, val_loader, num_epochs_1=100, num_epochs_2=50, \n",
    "                patience1=10, patience2=20, mask_x_true=True, mask_type=3, num_layers_to_freeze=1, filename='model'):\n",
    "    \n",
    "    def train_step(loader, optimizer=None, alpha=1.0):\n",
    "        \"\"\"Runs a training or validation step.\"\"\"\n",
    "        is_train = optimizer is not None\n",
    "        model.train() if is_train else model.eval()\n",
    "        total_loss, total_f1, total_nonzero_loss, total_presence_loss = 0.0, 0.0, 0.0, 0.0\n",
    "        y_true = np.zeros(X_train.size if is_train else X_val.size, dtype=np.float32)\n",
    "        y_pred = np.zeros(X_train.size if is_train else X_val.size, dtype=np.float32)\n",
    "        idx = 0\n",
    "        for X_batch, _ in loader:\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            latent, decoded1, decoded2, _ = model(X_batch)  \n",
    "            loss_presence = nn.BCEWithLogitsLoss()(decoded1, (X_batch != 0).float())\n",
    "\n",
    "            mask = {\n",
    "                0: (X_batch != 0).float(),\n",
    "                1: torch.sigmoid(decoded1),\n",
    "                2: torch.sigmoid(decoded2),\n",
    "                3: torch.sigmoid(decoded1).detach()\n",
    "            }.get(mask_type, torch.ones_like(X_batch))\n",
    "\n",
    "            loss_nonzero = softly_masked_mse_loss(torch.sigmoid(decoded2), X_batch, mask, mask_x_true=mask_x_true)\n",
    "            loss = alpha * loss_presence + (1 - alpha) * loss_nonzero\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() \n",
    "            total_presence_loss += loss_presence.item() \n",
    "            total_nonzero_loss += (loss_nonzero * torch.sum(mask)).item()\n",
    "\n",
    "            y_true[idx:idx + X_batch.numel()] = X_batch.cpu().flatten().numpy()\n",
    "            y_pred[idx:idx + X_batch.numel()] = torch.sigmoid(decoded1).detach().cpu().flatten().numpy()\n",
    "            idx += X_batch.numel()\n",
    "\n",
    "        num_nonzero = (X_train!=0).sum() if is_train else (X_val != 0).sum()\n",
    "        num_batch = len(loader)\n",
    "        return {\n",
    "            \"loss\": total_loss / num_batch,\n",
    "            \"presence_loss\": total_presence_loss / num_batch,\n",
    "            \"nonzero_loss\": total_nonzero_loss / num_nonzero,\n",
    "            \"f1\": f1_score((y_true >= 0.5).astype(int), (y_pred >= 0.5).astype(int)), \n",
    "            \"auroc\": roc_auc_score((y_true >= 0.5).astype(int), y_pred), \n",
    "            \"average_precision\": average_precision_score((y_true >= 0.5).astype(int), y_pred)\n",
    "        }\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        params = {\n",
    "            \"model_name\": model_name, \"mask_x_true\": mask_x_true, \"mask_type\": mask_type,\n",
    "            \"num_layers_to_freeze\": num_layers_to_freeze, \"latent_dim\": latent_dim,\n",
    "            \"dropout\": dropout, \"num_epochs_1\": num_epochs_1, \"patience1\": patience1,\n",
    "            \"num_epochs_2\": num_epochs_2, \"patience2\": patience2, \"ALPHA\": ALPHA\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Training Phase 1: Presence Prediction\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "        min_val_loss, best_model, early_stopping_counter = float('inf'), None, 0\n",
    "\n",
    "        for epoch in range(num_epochs_1):\n",
    "            train_metrics = train_step(train_loader, optimizer)\n",
    "            val_metrics = train_step(val_loader)\n",
    "\n",
    "            # Log metrics\n",
    "            for key, value in train_metrics.items():\n",
    "                mlflow.log_metric(f\"train_{key}\", value, step=epoch)\n",
    "            for key, value in val_metrics.items():\n",
    "                mlflow.log_metric(f\"val_{key}\", value, step=epoch)\n",
    "\n",
    "            scheduler.step(val_metrics[\"loss\"])\n",
    "            mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{model_name} Phase 1 Epoch {epoch+1}/{num_epochs_1}, Train Loss: {train_metrics['loss']:.4f}, Val Loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "            stop, min_val_loss, best_model, early_stopping_counter = early_stopping(epoch, val_metrics[\"loss\"], min_val_loss, best_model, model, early_stopping_counter, patience1)\n",
    "            if stop:\n",
    "                break\n",
    "\n",
    "        # Load best model & freeze encoder layers\n",
    "        model.load_state_dict(best_model)\n",
    "        for i, (name, param) in enumerate(model.encoder.named_parameters()):\n",
    "            if i < num_layers_to_freeze:\n",
    "                param.requires_grad = False\n",
    "                print(f\"Freezing {name}\")\n",
    "\n",
    "        # Training Phase 2: Nonzero Value Prediction\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "        min_val_loss, early_stopping_counter = float('inf'), 0\n",
    "\n",
    "        for epoch in range(num_epochs_2):\n",
    "            alpha = max(ALPHA, 1 - epoch / 20)\n",
    "            train_metrics = train_step(train_loader, optimizer, alpha)\n",
    "            val_metrics = train_step(val_loader, None, alpha)\n",
    "\n",
    "            # Log metrics\n",
    "            for key, value in train_metrics.items():\n",
    "                mlflow.log_metric(f\"train_{key}\", value, step=epoch)\n",
    "            for key, value in val_metrics.items():\n",
    "                mlflow.log_metric(f\"val_{key}\", value, step=epoch)\n",
    "\n",
    "            scheduler.step(val_metrics[\"loss\"])\n",
    "            mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{model_name} Phase 2 Epoch {epoch+1}/{num_epochs_2}, Train Loss: {train_metrics['loss']:.4f}, Val Loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "            if epoch > 20:\n",
    "                stop, min_val_loss, best_model, early_stopping_counter = early_stopping(epoch, val_metrics[\"loss\"], min_val_loss, best_model, model, early_stopping_counter, patience2)\n",
    "                if stop:\n",
    "                    break\n",
    "\n",
    "        mlflow.pytorch.log_model(model, filename)\n",
    "\n",
    "        return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8c97fb-4f9c-400e-adec-3dd98641bcd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941c8ef6-0e7c-45f6-9bed-cf77bdc664f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a88e597-905c-4d72-ab0b-c0c972fbf385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parameters to define before each experiment\n",
    "\n",
    "latent_dim = 20 # García-Jiménez et al. 2021 used latent_dim 10 to represent 717 taxa https://academic.oup.com/bioinformatics/article/37/10/1444/5988714\n",
    "\n",
    "num_epochs_1 = 50\n",
    "num_epochs_2 = 200\n",
    "patience1 = 10\n",
    "patience2 = 10\n",
    "dropout = 0.1\n",
    "\n",
    "mask_x_true = False\n",
    "mask_type = 3\n",
    "\n",
    "num_layers_to_freeze = 1\n",
    "\n",
    "ALPHA = 0.9\n",
    "\n",
    "\n",
    "model_name = \"AttentionAEmid\"\n",
    "model = AttentionAEmid(input_dim, latent_dim, dropout=dropout)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6ed4f5-07b8-48e6-83df-3a5dbcbad42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=X_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8b0eed-d565-4130-ae39-d9dec1514251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for ALPHA in np.arrange(0, 1, 0.1):\n",
    "filename = get_model_filename(model_name, mask_x_true, mask_type, num_layers_to_freeze, latent_dim, dropout, num_epochs_1, patience1, num_epochs_2, patience2, ALPHA)\n",
    "print(filename)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(f\"model/{filename}_best_model.pth\"):\n",
    "    print('model already trained, load model and latents')    \n",
    "    best_model = torch.load(f\"model/{filename}_best_model.pth\")\n",
    "    model.load_state_dict(best_model)\n",
    "    latent = np.loadtxt(f\"latents/latents_{filename}.txt\")\n",
    "else:\n",
    "    print('model not trained, train model')\n",
    "    best_model = train_model(model, model_name, train_loader, val_loader, num_epochs_1=num_epochs_1, num_epochs_2=num_epochs_2, patience1=patience1, patience2=patience2, mask_x_true=mask_x_true, num_layers_to_freeze=num_layers_to_freeze, filename=filename)\n",
    "\n",
    "    # lst_train_loss, lst_val_loss, lst_train_presence_loss, lst_val_presence_loss, lst_train_nonzero_loss, lst_val_nonzero_loss, lst_train_f1, lst_val_f1, best_model = train_results\n",
    "\n",
    "    # history = {\n",
    "    #     \"train_loss\": np.array(lst_train_loss),\n",
    "    #     \"val_loss\": np.array(lst_val_loss),\n",
    "    #     \"train_presence_loss\": np.array(lst_train_presence_loss),\n",
    "    #     \"val_presence_loss\": np.array(lst_val_presence_loss),\n",
    "    #     \"train_nonzero_loss\": np.array(lst_train_nonzero_loss),\n",
    "    #     \"val_nonzero_loss\": np.array(lst_val_nonzero_loss),\n",
    "    #     \"train_f1_score\": np.array(lst_train_f1),\n",
    "    #     \"val_f1_score\": np.array(lst_val_f1),\n",
    "    # }\n",
    "\n",
    "    # np.save(f\"history/history_{filename}.npy\", history)\n",
    "    torch.save(best_model, f\"model/{filename}_best_model.pth\")\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "\n",
    "t1 = time.time()\n",
    "print(f'\\t\\t ------------ model processed, time used = {t1 - t0} seconds ------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0afdde-602e-47f6-a6f2-2ed71641290d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## model computing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e291ea12-306d-435f-8e9f-af95ba8fe795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "sudo apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb11b91-7092-4082-bd77-39cef0d99d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "# output = model(X_test_tensor)\n",
    "# make_dot(output, params=dict(model.named_parameters())).render(model_name, format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd8262d-fea1-4559-b200-6cf6739265ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot_training_history(history).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3319380-e90d-4752-ab3c-eb042e60a74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93064428-793b-4875-a1e2-d8c542f2516b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply model on test set\n",
    "\n",
    "print(filename)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # X_train_latent = model(X_train_tensor)[0].cpu().detach().numpy()\n",
    "    model_output = model(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7277ad3f-9918-43e0-bbb7-f069eb67574d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get latents, reconstructions and metrics\n",
    "\n",
    "latent = model_output[0].cpu().detach().numpy()\n",
    "np.savetxt(f\"latents/latents_{filename}.txt\", latent)\n",
    "\n",
    "decoded1_sigmoid = torch.sigmoid(model_output[1]).cpu().detach().numpy()  # prob of presence\n",
    "recon_presence = (decoded1_sigmoid >= 0.5).astype(int)\n",
    "recon_nonzero = torch.sigmoid(model_output[2]).cpu().detach().numpy()\n",
    "mask_options = {\n",
    "    0: (X_test != 0).astype(float),\n",
    "    1: decoded1_sigmoid,\n",
    "    2: recon_nonzero,\n",
    "    3: decoded1_sigmoid\n",
    "}\n",
    "softmask = mask_options.get(mask_type, ValueError(\"Invalid mask_type\"))\n",
    "\n",
    "\n",
    "predicted_mask = softmask.copy() # or recon_presence ????\n",
    "predicted_mask[predicted_mask < 0.5] = 0\n",
    "# reconstructed = recon_nonzero * predicted_mask # or recon_presence ????\n",
    "reconstructed = recon_nonzero * recon_presence\n",
    "\n",
    "f1_test = f1_score(X_test.flatten() > 0, recon_presence.flatten())\n",
    "x_true_masked_mse_test = ((X_test != 0) * (recon_nonzero - X_test) ** 2).sum() / (X_test != 0).sum()\n",
    "soft_masked_mse_test = ((softmask * recon_nonzero - X_test) ** 2).sum() / (X_test != 0).sum()\n",
    "reconstructed_mse_test = ((X_test - reconstructed) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eac5fef-9348-435e-86d2-50df55474496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstructed_distribution(softmask, recon_nonzero, f1_test, x_true_masked_mse_test, soft_masked_mse_test, reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5865d6e8-85a9-4cdc-a177-8590cd283c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Evaluation of Presence/Absence Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46266bb4-abdb-43af-bf2f-1bef7b0373d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_true = (X_test != 0).astype(int).flatten()\n",
    "y_pred = recon_presence.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f568bf-5b21-4a82-88d4-a49b4bf4e58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b401ede6-4a96-4bce-a6f0-2b93fb1a5684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_auc(y_true, y_pred).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51180ac-d460-4ad8-a357-e75030a79e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Evaluation of Nonzero Values Reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f143b6e1-e2f9-4da2-a2f6-b8855e527c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scatter_plot_nonzero(X_test, recon_nonzero, softmask, soft_masked_mse_test, x_true_masked_mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860caf38-a233-45d8-9710-973937253220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "r2 = r2_score(X_test.flatten(), reconstructed.flatten())\n",
    "plt.figure(figsize=(4.5, 4 ))\n",
    "plt.scatter(X_test.flatten(), reconstructed.flatten(), alpha=0.1, s=1)  # final reconstructed value vs X_true\n",
    "plt.plot([X_test.min(), X_test.max()], [X_test.min(), X_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('X_test')\n",
    "plt.ylabel('reconstructed')\n",
    "plt.title('reconstructed vs X_test')\n",
    "plt.text(0.08, 0.95, f'F1 Score = {f1_test:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "plt.text(0.08, 0.85, f'R2 = {r2:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "plt.text(0.08, 0.75, f'MSE = {reconstructed_mse_test:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c3888e-be1a-40d1-bdd7-467c4c89a0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a hexbin plot for the reconstructed vs X_test\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Filter out the values at (0, 0)\n",
    "mask_origin = (X_test.flatten() != 0) | (reconstructed.flatten() != 0)\n",
    "X_test_filtered = X_test.flatten()[mask_origin]\n",
    "reconstructed_filtered = reconstructed.flatten()[mask_origin]\n",
    "\n",
    "plt.hexbin(X_test_filtered, reconstructed_filtered, gridsize=50, cmap='Blues', mincnt=1, vmin=0, vmax=3000)\n",
    "plt.colorbar(label='count in bin')\n",
    "plt.plot([X_test.min(), X_test.max()], [X_test.min(), X_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('X_test')\n",
    "plt.ylabel('reconstructed')\n",
    "plt.title('Hexbin plot, removing (0, 0)')\n",
    "plt.text(0.08, 0.95, f'F1 Score = {f1_test:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "plt.text(0.08, 0.85, f'R2 = {r2:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "plt.text(0.08, 0.75, f'MSE = {reconstructed_mse_test:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821ce1b4-a503-4976-a4d7-7b4829e865a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ce803b-2b8e-4254-a1e5-9c8ea24eea54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**reduce to 2D for visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7943ac5b-90c7-408e-8701-6f66f4364e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def perform_dimensionality_reduction(X_test, latent, test_idx, df_genus):\n",
    "    df_reduced = pd.DataFrame(index=df_genus.index[test_idx])\n",
    "\n",
    "    # PCA and t-SNE on X_test\n",
    "    pca_original = PCA(n_components=2)\n",
    "    df_reduced[['pca_original_dim_1', 'pca_original_dim_2']] = pca_original.fit_transform(X_test)\n",
    "    tsne_original = TSNE(n_components=2, random_state=42)\n",
    "    df_reduced[['tsne_original_dim_1', 'tsne_original_dim_2']] = tsne_original.fit_transform(X_test)\n",
    "\n",
    "    # PCA and t-SNE on latent\n",
    "    pca_latent = PCA(n_components=2)\n",
    "    df_reduced[['pca_latent_dim_1', 'pca_latent_dim_2']] = pca_latent.fit_transform(latent)\n",
    "    tsne_latent = TSNE(n_components=2, random_state=42)\n",
    "    df_reduced[['tsne_latent_dim_1', 'tsne_latent_dim_2']] = tsne_latent.fit_transform(latent)\n",
    "\n",
    "    print(\"PCA Latent Explained Variance Ratio:\", pca_latent.explained_variance_ratio_ * 100)\n",
    "    print(\"PCA Original Explained Variance Ratio:\", pca_original.explained_variance_ratio_ * 100)\n",
    "    print(\"t-SNE Latent KL Divergence:\", tsne_latent.kl_divergence_)\n",
    "    print(\"t-SNE Original KL Divergence:\", tsne_original.kl_divergence_)\n",
    "\n",
    "    return df_reduced\n",
    "# df_reduced = perform_dimensionality_reduction(X_test, latent, test_idx, df_genus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b4f068-8132-45ea-9cb7-5c2a273c9a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**load samples' metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239264ea-10a4-4e8a-a5bd-e8d194140103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"../data/metadata.txt\", sep='\\t', header=0, index_col=0)\n",
    "col_factors = ['age', 'antibiotics_current_use', 'gender', 'country', 'non_westernized', 'sequencing_platform', 'disease', \"study_condition\"]\n",
    "print(df_meta.shape)\n",
    "df_meta = df_meta.loc[df_genus.index, :]\n",
    "if df_meta.index.equals(df_genus.index):\n",
    "    meta_test = df_meta.iloc[test_idx, :]\n",
    "    meta_test = meta_test[col_factors]\n",
    "else:\n",
    "    raise ValueError(\"meta data and abundance data index mismatch\")\n",
    "\n",
    "meta_test['age'] = meta_test['age'].fillna(-1)\n",
    "meta_test['age'] = meta_test['age'].astype(int)\n",
    "# for col in col_factors:\n",
    "#     if meta_test[col].dtype == 'object':\n",
    "#         meta_test[col].fillna('missing', inplace=True)\n",
    "#     else:\n",
    "#         meta_test[col].fillna(-1, inplace=True)\n",
    "meta_test['is_healthy'] = ['healthy'if _ == 'healthy'  else 'disease'for _ in meta_test['disease']]\n",
    "meta_test['study_condition'] = ['control'if _ == 'control'  else 'case'for _ in meta_test['study_condition']]\n",
    "\n",
    "print(meta_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2393a8b7-965c-4985-878c-27ca52ec879f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_non_zero_counts = pd.DataFrame((X_test != 0).sum(axis=1), columns=['non_zero_counts'], index = df_genus.index[test_idx])\n",
    "col_factors = ['non_zero_counts', 'age', 'antibiotics_current_use', 'gender', 'country', 'non_westernized', 'sequencing_platform', 'is_healthy', \"study_condition\"]\n",
    "\n",
    "df_results = pd.concat([df_reduced, df_non_zero_counts, meta_test], axis=1)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e9e284-982b-449d-baf3-9d6b6339f459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_latents(df_results, col_factors,  'pca', show_vectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b909c5e-711e-4c9c-b170-f6f1abbbbfad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_latents(df_results, col_factors, 'tsne', show_vectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1d8f87-9f60-49e9-8bd6-498539bdadec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def plot_subplots(df_results, reduced_method='pca', show_vectors=False):\n",
    "#     fig, axes = plt.subplots(len(col_factors), 2, figsize=(15, 5 * len(col_factors)))\n",
    "\n",
    "#     for i, factor in enumerate(col_factors):\n",
    "#         sns.scatterplot(ax=axes[i, 0], x=f'{reduced_method}_original_dim_1', y=f'{reduced_method}_original_dim_2', hue=factor, data=df_results, s=10, alpha=0.5, legend='brief')\n",
    "#         axes[i, 0].set_title(f'{reduced_method.upper()} Original - Colored by {factor}')\n",
    "#         axes[i, 0].set_xlabel(f'{reduced_method}_original_dim_1')\n",
    "#         axes[i, 0].set_ylabel(f'{reduced_method}_original_dim_2')\n",
    "#         if show_vectors: \n",
    "#             add_vectors(axes[i, 0], df_results, pca_original, f'{reduced_method}_original_dim_1', f'{reduced_method}_original_dim_2', df_genus.columns, top=5)\n",
    "#         axes[i, 0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#         sns.scatterplot(ax=axes[i, 1], x=f'{reduced_method}_latent_dim_1', y=f'{reduced_method}_latent_dim_2', hue=factor, data=df_results, s=10, alpha=0.5, legend='brief')\n",
    "#         axes[i, 1].set_title(f'{reduced_method.upper()} Latent - Colored by {factor}')\n",
    "#         axes[i, 1].set_xlabel(f'{reduced_method}_latent_dim_1')\n",
    "#         axes[i, 1].set_ylabel(f'{reduced_method}_latent_dim_2')\n",
    "#         if show_vectors: \n",
    "#             add_vectors(axes[i, 1], df_results, pca_latent, f'{reduced_method}_latent_dim_1', f'{reduced_method}_latent_dim_2', range(latent.shape[1]), top=5)\n",
    "#         axes[i, 1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        \n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# plot_subplots(df_results, 'pca', show_vectors=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cf7f28-4a11-4b63-895b-95ba3a926080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot_subplots(df_results, 'tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b63ea4a-3c61-4da2-a3d7-68e4cc664c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# save table in catalog for age pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8f535f-11cb-43b3-8257-99a5919b9fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_test.reset_index(drop=True, inplace=True)\n",
    "meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107783b3-0629-4224-b076-9d3d4da090c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2d1645-34c7-44fc-817e-0c8214b5170f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine latent with metadata\n",
    "df_encoded_meta = pd.concat([meta_test.reset_index(drop=True), pd.DataFrame(latent)], axis=1)\n",
    "df_encoded_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7a4849-6c4b-461b-b2bf-75a36f2a8fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG onesource_datascience_sbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "161a6dcc-2af9-4248-9d89-4c70abe6897d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df_encoded_meta = spark.createDataFrame(df_encoded_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f22f5c-bcec-4257-9b1e-69acdbb3c699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS microbiota.encoded\")\n",
    "spark_df_encoded_meta.write.mode(\"overwrite\").saveAsTable(\"microbiota.encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c9edbf-8be8-41aa-b24c-ebfc42aaa328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_encoded_meta_healthy = df_encoded_meta[df_encoded_meta['is_healthy'] == 1]\n",
    "print(df_encoded_meta_healthy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ae1aed6-94fd-4e89-9cff-ac503225f8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df_encoded_"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8795884907347292,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_cMD_Autoencoders_for_microbiota",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
