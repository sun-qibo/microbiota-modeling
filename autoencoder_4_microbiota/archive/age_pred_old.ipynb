{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a25185e-84c5-494f-a802-ab985b47aa81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b474cb72-725a-483f-813a-0f1bca8a8c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, losses, callbacks, regularizers\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras import backend as K\n",
    "# import os\n",
    "\n",
    "# # Ensure reproducibility\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# # Check GPU availability\n",
    "# device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n",
    "# # Example input (X) and target (y) for demonstration purposes\n",
    "# n_samples = 16000\n",
    "# n_features = 1641\n",
    "# X = np.random.rand(n_samples, n_features).astype(np.float32)\n",
    "# y = np.random.rand(n_samples).astype(np.float32)\n",
    "\n",
    "# # Standardize features and target\n",
    "# scaler_X = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "# X = scaler_X.fit_transform(X)\n",
    "# y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# # Parameters\n",
    "# batch_size = 256\n",
    "# latent_dim = 50\n",
    "# n_splits = 5\n",
    "\n",
    "# def build_shallow_autoencoder(input_dim):\n",
    "#     inputs = layers.Input(shape=(input_dim,))\n",
    "#     encoded = layers.Dense(512, activation='relu')(inputs)\n",
    "#     latent = layers.Dense(latent_dim, activation='relu')(encoded)\n",
    "#     decoded = layers.Dense(512, activation='relu')(latent)\n",
    "#     outputs = layers.Dense(input_dim)(decoded)\n",
    "#     autoencoder = models.Model(inputs, outputs)\n",
    "#     encoder = models.Model(inputs, latent)\n",
    "#     return autoencoder, encoder\n",
    "\n",
    "# def build_deep_autoencoder(input_dim):\n",
    "#     inputs = layers.Input(shape=(input_dim,))\n",
    "#     encoded = layers.Dense(1024, activation='relu')(inputs)\n",
    "#     encoded = layers.Dense(512, activation='relu')(encoded)\n",
    "#     latent = layers.Dense(latent_dim, activation='relu')(encoded)\n",
    "#     decoded = layers.Dense(512, activation='relu')(latent)\n",
    "#     decoded = layers.Dense(1024, activation='relu')(decoded)\n",
    "#     outputs = layers.Dense(input_dim)(decoded)\n",
    "#     autoencoder = models.Model(inputs, outputs)\n",
    "#     encoder = models.Model(inputs, latent)\n",
    "#     return autoencoder, encoder\n",
    "\n",
    "# def build_variational_autoencoder(input_dim):\n",
    "#     inputs = layers.Input(shape=(input_dim,))\n",
    "#     encoded = layers.Dense(512, activation='relu')(inputs)\n",
    "#     z_mean = layers.Dense(latent_dim)(encoded)\n",
    "#     z_log_var = layers.Dense(latent_dim)(encoded)\n",
    "\n",
    "#     def sampling(args):\n",
    "#         z_mean, z_log_var = args\n",
    "#         epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#     z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "#     decoder_input = layers.Dense(512, activation='relu')(z)\n",
    "#     outputs = layers.Dense(input_dim)(decoder_input)\n",
    "\n",
    "#     vae = models.Model(inputs, outputs)\n",
    "#     kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "#     vae.add_loss(kl_loss)\n",
    "#     return vae, models.Model(inputs, z_mean)\n",
    "\n",
    "# def build_transformer(input_dim):\n",
    "#     inputs = layers.Input(shape=(input_dim,))\n",
    "#     x = layers.Reshape((input_dim, 1))(inputs)\n",
    "#     x = layers.MultiHeadAttention(num_heads=8, key_dim=4)(x, x)\n",
    "#     x = layers.GlobalAveragePooling1D()(x)\n",
    "#     latent = layers.Dense(latent_dim, activation='relu')(x)\n",
    "#     outputs = layers.Dense(1)(latent)\n",
    "#     transformer = models.Model(inputs, outputs)\n",
    "#     return transformer\n",
    "\n",
    "# def train_and_evaluate_model(model_builder, X, y, model_name, kfold):\n",
    "#     results = []\n",
    "#     latent_representations = []\n",
    "#     os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "#         print(f\"Training fold {fold + 1} of {n_splits} for {model_name}\")\n",
    "#         X_train, X_val = X[train_idx], X[val_idx]\n",
    "#         y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "#         with tf.device(device):\n",
    "#             model, encoder = model_builder(X.shape[1])\n",
    "#             model.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "#             es = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "#             csv_logger = callbacks.CSVLogger(os.path.join(model_name, f\"fold_{fold + 1}_log.csv\"))\n",
    "\n",
    "#             history = model.fit(\n",
    "#                 X_train, X_train if \"autoencoder\" in model_name.lower() else y_train,\n",
    "#                 validation_data=(X_val, X_val if \"autoencoder\" in model_name.lower() else y_val),\n",
    "#                 epochs=100,\n",
    "#                 batch_size=batch_size,\n",
    "#                 callbacks=[es, csv_logger],\n",
    "#                 verbose=1\n",
    "#             )\n",
    "\n",
    "#             # Save model and latent representations\n",
    "#             model.save(os.path.join(model_name, f\"fold_{fold + 1}.h5\"))\n",
    "#             if \"autoencoder\" in model_name.lower():\n",
    "#                 latent_representations.append(encoder.predict(X))\n",
    "#             else:\n",
    "#                 results.append(model.evaluate(X_val, y_val, verbose=0))\n",
    "\n",
    "#     return results, latent_representations\n",
    "\n",
    "# # K-Fold cross-validator\n",
    "# kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# # Train models\n",
    "# shallow_results, shallow_latents = train_and_evaluate_model(build_shallow_autoencoder, X, y, \"Shallow_Autoencoder\", kfold)\n",
    "# deep_results, deep_latents = train_and_evaluate_model(build_deep_autoencoder, X, y, \"Deep_Autoencoder\", kfold)\n",
    "# vae_results, vae_latents = train_and_evaluate_model(build_variational_autoencoder, X, y, \"Variational_Autoencoder\", kfold)\n",
    "# transformer_results, _ = train_and_evaluate_model(build_transformer, X, y, \"Transformer\", kfold)\n",
    "\n",
    "# print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981eedd4-6e50-4ed6-9ea5-25882dad3f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ae_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f99260-bb64-43ab-a2ce-2b8fc0eb90ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848bc125-c142-4aac-92f7-8c27110b0ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f34807c-70a7-4567-b5c0-bfea24dfff2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloader, num_epochs=100, learning_rate=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, _ in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_data = model(data)\n",
    "            loss = criterion(recon_data, data)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z_means = []\n",
    "    for data, _ in dataloader:\n",
    "        mu, logvar = model.encode(data)\n",
    "        z_means.append(mu)\n",
    "    z_means = torch.cat(z_means, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "793ab985-9506-4a75-ad62-97add4ca97cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloader, num_epochs=100, learning_rate=1e-3, loss_function=nn.MSELoss()):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, _ in dataloader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(model, VAE):\n",
    "                recon_batch, mu, logvar = model(data)\n",
    "                loss = vae_loss_function(recon_batch, data, mu, logvar)\n",
    "            else:\n",
    "                recon_batch = model(data)\n",
    "                loss = loss_function(recon_batch, data)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_loss = train_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {val_loss}')\n",
    "        \n",
    "        early_stopping(val_loss)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_means = []\n",
    "        for data, _ in dataloader:\n",
    "            mu, logvar = model.encode(data)\n",
    "            z_means.append(mu)\n",
    "        z_means = torch.cat(z_means, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4695e959-8d65-48b9-8789-404f36bc2e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss)\n",
    "        elif val_loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'best_autoencoder_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd74cd88-838f-4e4e-b3f9-176d5e46bd15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc8228b-24fb-4ac9-84e1-6872ccc3f04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y = pd.read_csv(\"../data/age.csv\", header=None)\n",
    "X = pd.read_csv(\"../data/processed_log_abundance.csv\", header=None, index_col=0)\n",
    "\n",
    "display(y)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "971e9c7c-8df8-41d4-b6ce-5608105412a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87ee8c17-1b9d-4a32-9761-63745c573328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train Deep Autoencoder\n",
    "deep_ae = DeepAE(input_dim).to(device)\n",
    "train_autoencoder(deep_ae, dataloader)\n",
    "\n",
    "# Train Shallow Autoencoder\n",
    "shallow_ae = ShallowAE(input_dim).to(device)\n",
    "train_autoencoder(shallow_ae, dataloader)\n",
    "\n",
    "# Train VAE\n",
    "vae = VAE(input_dim, hidden_dim=512, latent_dim=64).to(device)\n",
    "train_autoencoder(vae, dataloader, loss_function=vae_loss_function)\n",
    "\n",
    "# Train Convolutional Autoencoder (reshape data as needed)\n",
    "# conv_ae = ConvAE().to(device)\n",
    "# train_autoencoder(conv_ae, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c9056b-ea71-4ab8-8798-af0315ff1599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_latent_representations(model, dataloader):\n",
    "    model.load_state_dict(torch.load('best_autoencoder_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_means = []\n",
    "        for data, _ in dataloader:\n",
    "            if isinstance(model, VAE):\n",
    "                mu, logvar = model.encode(data)\n",
    "                z_means.append(mu)\n",
    "            else:\n",
    "                z = model.encoder(data)\n",
    "                z_means.append(z)\n",
    "        z_means = torch.cat(z_means, dim=0)\n",
    "    return z_means\n",
    "\n",
    "# Example usage\n",
    "z_means = extract_latent_representations(deep_ae, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e4e3e62-3d59-49ba-a8e2-51fcdb16f035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for sklearn\n",
    "z_means_np = z_means.numpy()\n",
    "y_train_np = y_train.numpy()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(z_means_np, y_train_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "age_pred_old",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
