{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc33de9c-87cc-4e5a-bb80-296cc9cc2574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Prior knowledge\n",
    "1. diversity : increase in young adults, plateaus arond 40, dicrease in older adults\n",
    "2. older adults decrease in beneficial bacteria and increase in harmful bacteria\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "De La Cuesta-Zuluaga, J., Kelley, S., Chen, Y., Escobar, J., Mueller, N., Ley, R., McDonald, D., Huang, S., Swafford, A., Knight, R., & Thackray, V. (2019). Age- and Sex-Dependent Patterns of Gut Microbial Diversity in Human Adults. mSystems, 4. https://doi.org/10.1128/mSystems.00261-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "be8ef31f-36cd-4738-8f98-41b008a1ebba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3821af5e-db76-4517-a959-ffe0ccf5eeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263045b6-ae3a-4c2b-aa94-ff4b4c2e15d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3c0791-eec3-4363-a464-a9f052d2be00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef114a50-fe08-4548-9306-3b2bcc8d6be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_age = pd.read_csv(\"../data/age.csv\", header=None, index_col=0, sep='\\t')\n",
    "df_age = df_age[df_age[1]>=18]  # todo remove after test \n",
    "# y = df_age.to_numpy().reshape(-1, 1).flatten()\n",
    "\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0468817f-3744-4f1a-b1f6-286d822602a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# X = pd.read_csv(\"../data/processed_log_drop08_scaled.csv\", header=0, index_col=0, sep='\\t').loc[df_age.index, :].to_numpy()\n",
    "df_genus = pd.read_csv(\"../data/processed_genus_log_drop08_scaled.csv\", header=0, index_col=0, sep='\\t').loc[df_age.index, :]\n",
    "X = df_genus.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1e873d-2dd4-487e-98cd-23df12805a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diversity_genus = df_genus.astype(bool).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3434b948-4533-4578-b03e-56ae93245afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_age[1], y=diversity_genus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f833d2-7adf-4a10-adbd-d152c9b1b10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diversity_genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03e22d3-60dc-48fe-90c6-565ca9757fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_to_drop = diversity_genus[diversity_genus<10].index\n",
    "df_genus = df_genus.drop(sample_to_drop)\n",
    "df_age = df_age.drop(sample_to_drop)\n",
    "X = df_genus.to_numpy()\n",
    "y = df_age.to_numpy().reshape(-1, 1).flatten()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b965fa-fb32-47eb-8649-5dad2d8fe3e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_class = y//10\n",
    "y_class[y_class==9] = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a824e0f6-478c-495e-a4ee-1d7d01292843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, stratify=y_class, random_state=42)  # split the data once so that index keeps the same for different types of X\n",
    "test_idx[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85580ce2-e21b-497a-8cd3-2e1d574f581b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "[3070, 4428, 2088, 328, 675, 563, 555, 3960, 3624, 2427]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093021e5-6218-4b3d-9deb-58e6b8975c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# Convert to tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "print(X_train.max())\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1615d7c-9544-413f-b69e-154f417e95f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the proportion of zeros in X\n",
    "p_zero = (X_train == 0).mean()\n",
    "print(p_zero)\n",
    "print(p_zero/(1-p_zero))\n",
    "print(1/(1-p_zero))\n",
    "print(1/p_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782ebef4-c9b4-41ec-bf8a-afbcbe672eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nrow = 1\n",
    "ncol = 2\n",
    "i = 1\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(nrow, ncol, i)\n",
    "sns.histplot(X_test.flatten(), bins=10)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(0, 5e5)\n",
    "plt.title('Test Set - Original Distribution')\n",
    "i+=1\n",
    "\n",
    "plt.subplot(nrow, ncol, i )\n",
    "plt.ylim(0, 10000)\n",
    "sns.histplot(X_test.flatten(), bins=100)\n",
    "plt.title('Test Set - Original NonZero Values Distribution')\n",
    "i+=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bcc55c-994d-44e4-af09-f5a0c3a6c69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9f6793-9289-48f5-8292-c44fe10e38be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from models_attention_with_regressor_head import *\n",
    "\n",
    "# todo  add noise (e.g., zero out random values) to the input during training and train the network to reconstruct the original data. This encourages the model to learn robust features despite the sparse noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88781587-ddaf-4743-87c3-a734227fb504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce8e71e-ce4f-4d2e-9005-be6f0929836d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from loss_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ca5cbb-7832-43e7-9c34-a50596e656be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469773c4-f442-4479-a36d-22fa808136ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train_model(model, model_name, train_loader, test_loader, num_epochs_1=100, num_epochs_2 = 50, patience=10, gamma=0.1, mask_x_true=False):\n",
    "    lst_train_nonzero_loss = []\n",
    "    lst_train_presence_loss = []\n",
    "    lst_train_pred_loss = []\n",
    "    lst_train_loss = []\n",
    "    lst_train_f1 = []\n",
    "    lst_val_nonzero_loss = []\n",
    "    lst_val_presence_loss = []\n",
    "    lst_val_pred_loss = []\n",
    "    lst_val_loss = []\n",
    "    lst_val_f1 = []\n",
    "    ######################################\n",
    "    #### training for presence by BCE ####\n",
    "    ######################################\n",
    "\n",
    "    min_val_loss = float('inf')\n",
    "    best_model = None \n",
    "    early_stopping_counter = 0\n",
    "    optimizer1 = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs_1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pred_loss = 0.0\n",
    "        train_f1 = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_pred_loss = 0.0\n",
    "        val_f1 = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer1.zero_grad()\n",
    "            latent, decoded1, decoded2, predicted = model(X_batch)\n",
    "            loss_presence = nn.BCEWithLogitsLoss(reduction='mean')(decoded1, (X_batch !=0).float())\n",
    "            loss_pred = nn.MSELoss()(predicted, y_batch)\n",
    "            loss_presence.backward()\n",
    "            optimizer1.step()\n",
    "            train_loss += loss_presence.item() * len(X_batch)\n",
    "            train_pred_loss += loss_pred.item()\n",
    "            train_f1 += get_f1_score(decoded1, X_batch)\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_pred_loss = train_pred_loss/len(train_loader)\n",
    "        lst_train_loss.append(train_loss)\n",
    "        lst_train_presence_loss.append(train_loss)\n",
    "        lst_train_f1.append(train_f1/len(train_loader))\n",
    "        # lst_train_pred_loss.append(train_pred_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                latent, decoded1, decoded2, predicted = model(X_batch)\n",
    "                loss_presence = nn.BCEWithLogitsLoss(reduction='mean')(decoded1, (X_batch !=0).float())\n",
    "                loss_pred = nn.MSELoss()(predicted, y_batch)\n",
    "                val_loss += loss_presence.item() * len(X_batch)\n",
    "                val_pred_loss += loss_pred.item()\n",
    "                val_f1 += get_f1_score(decoded1, X_batch)\n",
    "            val_loss = val_loss/len(test_dataset)\n",
    "            val_pred_loss = val_pred_loss/len(test_loader)\n",
    "            lst_val_loss.append(val_loss)\n",
    "            lst_val_presence_loss.append(val_loss)\n",
    "            lst_val_f1.append(val_f1/ len(test_loader))\n",
    "            # lst_val_pred_loss.append(val_pred_loss)\n",
    "            \n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'{model_name} Presence Epoch {epoch+1}/{num_epochs_1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping at epoch\", epoch+1)\n",
    "                break\n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    #### training for nonzero values by masked MSE ####\n",
    "    ###################################################\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # freeze encoder parameters\n",
    "    last_layer_id = [name.split('.')[0] for name, _ in model.named_parameters()][-1]\n",
    "    for name, param in model.encoder.named_parameters():\n",
    "        if not last_layer_id in name:  # freeze all but last layer\n",
    "        # if '0' in name:  # freeze first layer\n",
    "            print(f\"freeze {name} of encoder\")\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    min_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    optimizer2 = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    for epoch in range(num_epochs_2):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_nonzero_loss = 0.0\n",
    "        train_presence_loss = 0.0\n",
    "        train_pred_loss = 0.0\n",
    "        train_f1 = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_nonzero_loss = 0.0\n",
    "        val_presence_loss = 0.0\n",
    "        val_pred_loss = 0.0\n",
    "        val_f1 = 0.0\n",
    "        alpha = max(0, 1 - epoch / 20)\n",
    "        # alpha = 0.5 * (1 + np.cos(np.pi * epoch/40))\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer2.zero_grad()\n",
    "            latent, decoded1, decoded2, predicted = model(X_batch)            \n",
    "            loss_presence = nn.BCEWithLogitsLoss(reduction='mean')(decoded1, (X_batch !=0).float())\n",
    "            # mask = (X_batch !=0).float()\n",
    "            # mask = torch.sigmoid(decoded1)\n",
    "            mask = torch.sigmoid(decoded1).detach()\n",
    "            # mask = torch.sigmoid(decoded2)\n",
    "            loss_non_zero = softly_masked_mse_loss(torch.sigmoid(decoded2), X_batch, mask, mask_x_true=mask_x_true)\n",
    "            loss_pred = nn.MSELoss()(predicted, y_batch)\n",
    "            loss = (alpha * loss_presence + (1 - alpha) * loss_non_zero ) *(1 - gamma) + gamma * loss_pred\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            train_loss += loss.item() * len(X_batch)\n",
    "            train_nonzero_loss += (loss_non_zero * torch.sum(mask)).item()\n",
    "            train_presence_loss += loss_presence.item() * len(X_batch)\n",
    "            train_pred_loss += loss_pred.item()\n",
    "            train_f1 += get_f1_score(decoded1, X_batch)\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        lst_train_loss.append(train_loss)\n",
    "        lst_train_presence_loss.append(train_presence_loss/len(train_dataset))\n",
    "        lst_train_nonzero_loss.append(train_nonzero_loss/(X_train != 0).sum())\n",
    "        lst_train_pred_loss.append(train_pred_loss/len(train_loader))\n",
    "        lst_train_f1.append(train_f1/len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                latent, decoded1, decoded2, predicted = model(X_batch)\n",
    "                loss_presence = nn.BCEWithLogitsLoss(reduction='mean')(decoded1, (X_batch !=0).float())\n",
    "                # mask = (X_batch !=0).float()\n",
    "                # mask = torch.sigmoid(decoded1)\n",
    "                mask = torch.sigmoid(decoded1).detach()\n",
    "                # mask = torch.sigmoid(decoded2)\n",
    "                loss_non_zero = softly_masked_mse_loss(torch.sigmoid(decoded2), X_batch, mask,  mask_x_true=mask_x_true)\n",
    "                loss_pred = nn.MSELoss()(predicted, y_batch)\n",
    "                loss = (alpha * loss_presence + (1 - alpha) * loss_non_zero) *(1 - gamma) + gamma * loss_pred\n",
    "                val_loss += loss.item() * len(X_batch)\n",
    "                val_nonzero_loss += (loss_non_zero * torch.sum(mask)).item()\n",
    "                val_presence_loss += loss_presence.item() * len(X_batch)\n",
    "                val_pred_loss += loss_pred.item()\n",
    "                val_f1 += get_f1_score(decoded1, X_batch)\n",
    "            val_loss = val_loss/len(test_dataset)\n",
    "            lst_val_loss.append(val_loss)\n",
    "            lst_val_presence_loss.append(val_presence_loss/len(test_dataset))\n",
    "            lst_val_nonzero_loss.append(val_nonzero_loss/(X_test != 0).sum())\n",
    "            lst_val_pred_loss.append(val_pred_loss/len(test_loader))\n",
    "            lst_val_f1.append(val_f1/len(test_loader))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'{model_name} NonZero Epoch {epoch+1}/{num_epochs_2}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 21:\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                best_model = model.state_dict()\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience *2:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "    return lst_train_loss, lst_val_loss, lst_train_presence_loss, lst_val_presence_loss, lst_train_nonzero_loss, lst_val_nonzero_loss, lst_train_pred_loss, lst_val_pred_loss, lst_train_f1, lst_val_f1, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941c8ef6-0e7c-45f6-9bed-cf77bdc664f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a88e597-905c-4d72-ab0b-c0c972fbf385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parameters to define before each experiment\n",
    "\n",
    "latent_dim = 20 # García-Jiménez et al. 2021 used latent_dim 10 to represent 717 taxa https://academic.oup.com/bioinformatics/article/37/10/1444/5988714\n",
    "\n",
    "num_epochs_1 = 50\n",
    "num_epochs_2 = 200\n",
    "patience = 10\n",
    "gamma = 0  # weight for regressor loss, between 0 and 1\n",
    "dropout = 0.1\n",
    "\n",
    "mask_x_true=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df4a832-44d1-456a-9965-96e3c599c66b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "models = [\n",
    "    AttentionAEmidconcat(input_dim, latent_dim, dropout=dropout), \n",
    "    # AttentionAEend(input_dim, latent_dim, dropout=dropout),\n",
    "    # AttentionAEmid(input_dim, latent_dim, dropout=dropout),\n",
    "    # AttentionAEbegin(input_dim, latent_dim, dropout=dropout),\n",
    "    # DeepShallowAutoencoder(input_dim, latent_dim),\n",
    "    # DeepShallowAutoencoder(input_dim, latent_dim),\n",
    "    # # ShallowVAE(input_dim, latent_dim),\n",
    "    # # DeepVAE(input_dim, latent_dim),\n",
    "    # ShallowAutoencoder(input_dim, latent_dim),\n",
    "    # DeepAutoencoder(input_dim, latent_dim),\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"AttentionAEmidconcat\",\n",
    "    # \"AttentionAEend\",\n",
    "    # \"AttentionAEmid\",\n",
    "    # \"AttentionAEbegin\",\n",
    "    # \"DeepShallowAE\",\n",
    "\n",
    "]\n",
    "\n",
    "lst_xgb_models = []\n",
    "\n",
    "# log_dirs = [f\"run/{model_name}\" for model_name in model_names]\n",
    "# dct_writer = dict()\n",
    "\n",
    "\n",
    "dct_history = dict()\n",
    "dct_y_pred = dict()\n",
    "dct_latent_vectors = dict()\n",
    "dct_metrics = dict()\n",
    "dct_presence = dict()\n",
    "dct_nonzero = dict()\n",
    "\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    # log_dir = f\"run/{model_name}\"\n",
    "    # dct_writer[model_name] = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(f\"Training {model_name}\")\n",
    "\n",
    "    model.to(device)\n",
    "    # dct_writer[model_name].add_graph(model, X_train_tensor)\n",
    "    # dct_writer[model_name].close()\n",
    "    \n",
    "\n",
    "    # training\n",
    "    lst_train_loss, lst_val_loss, lst_train_presence_loss, lst_val_presence_loss, lst_train_nonzero_loss, lst_val_nonzero_loss, lst_train_pred_loss, lst_val_pred_loss, lst_train_f1, lst_val_f1, best_model = train_model(model, model_name, train_loader, test_loader, num_epochs_1=num_epochs_1, num_epochs_2=num_epochs_2, patience=patience, gamma=gamma, mask_x_true=mask_x_true)\n",
    "\n",
    "    torch.save(best_model, f\"model/{model_name}_best_model.pth\")\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    t1 = time.time()\n",
    "    print(f'\\t\\t ------------ model trained, time used = {t1 - t0} seconds ------------')\n",
    "\n",
    "\n",
    "    print('\\nmodel evaluating')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_output = model(X_test_tensor)\n",
    "        X_test_latent = model_output[0].cpu().detach().numpy() \n",
    "        X_test_presence = torch.sigmoid(model_output[1]).cpu().detach().numpy() \n",
    "        X_test_nonzero = torch.sigmoid(model_output[2]).cpu().detach().numpy() \n",
    "        X_train_latent = model(X_train_tensor)[0].cpu().detach().numpy()\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(f'\\t\\t ------------ model applied on test set, time used = {t2 - t1} seconds ------------')\n",
    "    # save model outputs\n",
    "    dct_presence[model_name] = X_test_presence.flatten()\n",
    "    dct_nonzero[model_name] = X_test_nonzero.flatten()\n",
    "    dct_latent_vectors[model_name] = X_test_latent\n",
    "\n",
    "    # get and save metrics on test set\n",
    "    f1_test = f1_score(X_test.flatten()>0, X_test_presence.flatten()>0.5)\n",
    "    mse_test = ((X_test - X_test_presence)**2).mean()\n",
    "    masked_mse_test = ((X_test!=0) * (X_test_nonzero - X_test)**2).sum()/(X_test!=0).sum()\n",
    "    soft_masked_mse_test = ((X_test_presence * X_test_nonzero -  X_test_presence * X_test)**2).sum()/X_test_presence.sum()\n",
    "    dct_metrics[model_name] = {'f1_test': f1_test, 'mse_test': mse_test, 'masked_mse_test': masked_mse_test, 'soft_masked_mse_test': soft_masked_mse_test}\n",
    "    \n",
    "    # Log losses\n",
    "    dct_history[model_name] = {\n",
    "            \"train_loss\": np.array(lst_train_loss),\n",
    "            \"val_loss\": np.array(lst_val_loss),\n",
    "            \"train_presence_loss\": np.array(lst_train_presence_loss),\n",
    "            \"val_presence_loss\": np.array(lst_val_presence_loss),\n",
    "            \"train_nonzero_loss\": np.array(lst_train_nonzero_loss),\n",
    "            \"val_nonzero_loss\": np.array(lst_val_nonzero_loss),\n",
    "            'train_pred_loss': np.array(lst_train_pred_loss),\n",
    "            \"val_pred_loss\": np.array(lst_val_pred_loss),\n",
    "            \"train_f1_score\": np.array(lst_train_f1),\n",
    "            \"val_f1_score\": np.array(lst_val_f1),\n",
    "        }\n",
    "    \n",
    "    # Train XGBoost model on latent features\n",
    "    print(\"\\nprediction using embedding by\", model_name)\n",
    "    xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=4)\n",
    "    xgb_model.fit(X_train_latent, y_train)  # Use the latent features as input for regression\n",
    "    lst_xgb_models.append(xgb_model)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    dct_y_pred[model_name] =  xgb_model.predict(X_test_latent)\n",
    "        \n",
    "    t3 = time.time()\n",
    "    print(f'\\t\\t ------------ XGB trained and tested, time used = {t3 - t2} seconds ------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3319380-e90d-4752-ab3c-eb042e60a74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8312d9b1-dbf9-4cfd-a5da-487ec3a1b5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ncol = 4\n",
    "nrow = len(model_names) \n",
    "if gamma != 0:\n",
    "    ncol += 1\n",
    "plt.figure(figsize=(4 * ncol, 3 * nrow))\n",
    "for i, (model_name, history) in enumerate(dct_history.items()):\n",
    "    plt.subplot(nrow, ncol,  ncol*i + 1)\n",
    "    plt.plot(history['train_loss'], '-', label=f'Train', color='blue', alpha=0.5)\n",
    "    plt.plot(history['val_loss'], '--', label=f'Validation', color='red', alpha=0.5)\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(nrow, ncol,  ncol*i + 2)\n",
    "    plt.plot(history['train_f1_score'], '-', label=f'Train', color='blue', alpha=0.5)\n",
    "    plt.plot(history['val_f1_score'], '--', label=f'Validation', color='red', alpha=0.5)\n",
    "    plt.title(f'{model_name} - F1 score')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "    plt.subplot(nrow, ncol,  ncol*i + 3)\n",
    "    plt.plot(history['train_presence_loss'], '-', label=f'Train', color='blue', alpha=0.5)\n",
    "    plt.plot(history['val_presence_loss'], '--', label=f'Validation', color='red', alpha=0.5)\n",
    "    plt.title(f'{model_name} - Presence Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(nrow, ncol,  ncol*i + 4)\n",
    "    plt.plot(history['train_nonzero_loss'], '-', label=f'Train', color='blue', alpha=0.5)\n",
    "    plt.plot(history['val_nonzero_loss'], '--', label=f'Validation', color='red', alpha=0.5)\n",
    "    plt.title(f'{model_name} - Non-zero Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    if gamma != 0:\n",
    "        plt.subplot(nrow,  ncol,  ncol*i + 5)\n",
    "        plt.plot(history['train_pred_loss'], '-', label=f'Train', color='blue', alpha=0.5)\n",
    "        plt.plot(history['val_pred_loss'], '--', label=f'Validation', color='red', alpha=0.5)\n",
    "        plt.title(f'{model_name} - Prediction Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4b6d20-1113-469a-8420-899c25b5e97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ncol = 3\n",
    "nrow = len(model_names) \n",
    "\n",
    "plt.figure(figsize=(5 * ncol, 3 * nrow))\n",
    "i = 1\n",
    "\n",
    "# plt.subplot(nrow, ncol, i)\n",
    "# sns.histplot(X_test.flatten(), bins=10)\n",
    "# plt.xlim(-0.05, 1.05)\n",
    "# plt.ylim(0, 5e5)\n",
    "# plt.title('Test Set - Original Distribution')\n",
    "# i+=1\n",
    "\n",
    "# plt.subplot(nrow, ncol, i )\n",
    "# plt.ylim(0, 10000)\n",
    "# sns.histplot(X_test.flatten(), bins=100)\n",
    "# plt.title('Test Set - Original NonZero Values Distribution')\n",
    "# i+=2\n",
    "\n",
    "# plt.subplot(nrow, ncol, i )\n",
    "# plt.xlim(0, 1)\n",
    "# # plt.ylim(0, 10000)\n",
    "# sns.histplot(X_test.flatten(), bins=100)\n",
    "# plt.title('Test Set - Original NonZero Values Distribution')\n",
    "# i+=1\n",
    "\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    X_test_presence = dct_presence[model_name]\n",
    "    X_test_nonzero = dct_nonzero[model_name]\n",
    "    f1 = dct_metrics[model_name]['f1_test']\n",
    "    mse_test = dct_metrics[model_name]['mse_test']\n",
    "    masked_mse_test = dct_metrics[model_name]['masked_mse_test']\n",
    "    soft_masked_mse_test = dct_metrics[model_name]['soft_masked_mse_test']\n",
    "\n",
    "    # plot the metrics\n",
    "    plt.subplot(nrow, ncol, i )\n",
    "    sns.histplot(X_test_presence *  X_test_nonzero , bins=10)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(0, 5e5)\n",
    "    plt.title(f'{model_name} - Reconstructed Presence Distribution')\n",
    "    plt.text(0.5, 300000, f'F1 score = {f1:.2f}', fontsize=14)\n",
    "    i+=1\n",
    "\n",
    "    plt.subplot(nrow, ncol, i )\n",
    "    sns.histplot(X_test_presence *  X_test_nonzero , bins=100)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(0, 10000)\n",
    "    plt.text(0.1, 6000, f'soft masked MSE = {soft_masked_mse_test:.3f}', fontsize=14)\n",
    "    plt.title(f'{model_name} - Reconstructed Values Distribution')\n",
    "    i+=1\n",
    "\n",
    "    plt.subplot(nrow, ncol, i )\n",
    "    sns.histplot(X_test_nonzero , bins=100)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(0, 10000)\n",
    "    plt.text(0.1, 6000, f'masked MSE = {masked_mse_test:.3f}', fontsize=14)\n",
    "    plt.title(f'{model_name} - Reconstructed non zero Values Distribution')\n",
    "    i+=1\n",
    "\n",
    "\n",
    "    # plt.subplot(nrow, ncol, i )\n",
    "    # sns.histplot(X_test_presence *  X_test_nonzero , bins=100)\n",
    "    # plt.xlim(0, 1)\n",
    "    # # plt.ylim(0, 16000)\n",
    "    # plt.title(f'{model_name} - Reconstructed NonZero values Distribution')\n",
    "    # plt.text(0.01, 10000, f'masked MSE = {masked_mse_test:.3f}', fontsize=14)\n",
    "    # i+=1\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2619a476-082c-4b1d-bbee-487892dcd758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for log_dir in log_dirs:\n",
    "#     %tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab40a888-d53d-42e5-89da-c45cc0509873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f404e3c-f835-4bb5-8d5d-68d3be5a5db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nrow = len(model_names)\n",
    "plt.figure(figsize=(5, 4 * nrow))\n",
    "\n",
    "\n",
    "for i, (model_name, y_pred) in enumerate(dct_y_pred.items()):\n",
    "    \n",
    "    y_pred = y_pred.squeeze()\n",
    "\n",
    "    plt.subplot(nrow, 1, i + 1)\n",
    "\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, s=6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(model_name)\n",
    "\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    plt.text(0.05, 0.95, f'R^2: {r2:.2f}\\nMSE: {mse:.2f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31275354-53ee-4e70-a2e0-d9423e69cbbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test_presence = dct_presence[model_names[0]]\n",
    "X_test_nonzero = dct_nonzero[model_names[0]] \n",
    "\n",
    "# X_test_presence = X_test_nonzero  # when encoder 1 was not changed during training stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "069715fe-0637-459d-a15a-e10262dc7fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate confusion matrix\n",
    "y_true = (X_test != 0).astype(int).flatten()\n",
    "y_pred = (X_test_presence >= 0.5).astype(int).flatten()\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix of X_test != 0 and X_test_presence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b401ede6-4a96-4bce-a6f0-2b93fb1a5684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Calculate Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "auprc = average_precision_score(y_true, y_pred)\n",
    "\n",
    "# Plot ROC and PRC curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# ROC curve\n",
    "ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# PRC curve\n",
    "ax2.plot(recall, precision, color='blue', lw=2, label=f'PRC curve (area = {auprc:.2f})')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve')\n",
    "ax2.legend(loc=\"lower left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f143b6e1-e2f9-4da2-a2f6-b8855e527c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(X_test.flatten(), X_test_nonzero.flatten())\n",
    "masked_mse = mean_squared_error(X_test.flatten()[X_test.flatten() > 0], X_test_nonzero.flatten()[X_test.flatten() > 0])\n",
    "softly_masked_mse = sum((X_test.flatten()*X_test_presence.flatten() - X_test_nonzero.flatten()*X_test_presence.flatten())**2)/sum(X_test_presence)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 4 * len(model_names)))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X_test, X_test_nonzero, alpha=0.1, s=6)\n",
    "plt.plot([X_test.min(), X_test.max()], [X_test.min(), X_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('X_test')\n",
    "plt.ylabel('X_test_nonzero')\n",
    "plt.title('Scatterplot of X_test_nonzero vs X_test')\n",
    "plt.text(0.05, 0.95, f'MSE: {mse:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X_test.flatten()[X_test.flatten() > 0], X_test_nonzero.flatten()[X_test.flatten() > 0], alpha=0.1, s=6)\n",
    "plt.plot([X_test.min(), X_test.max()], [X_test.min(), X_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('X_test')\n",
    "plt.ylabel('X_test_nonzero')\n",
    "plt.title('Scatterplot of X_test_nonzero vs X_test (Non-zero)')\n",
    "plt.text(0.05, 0.95, f'Masked MSE: {masked_mse:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(X_test.flatten(), X_test_nonzero.flatten()*X_test_presence.flatten(), alpha=0.1, s=6)\n",
    "plt.plot([X_test.min(), X_test.max()], [X_test.min(), X_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('X_test')\n",
    "plt.ylabel('X_test_nonzero')\n",
    "plt.title('Scatterplot of X_test_nonzero vs X_test (Non-zero)')\n",
    "plt.text(0.05, 0.95, f'Softly Masked MSE: {softly_masked_mse:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0d5eca-7c79-46e3-a52e-9711c0fc99aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(X_test != 0).sum().round(0), (X_test_presence).sum().round(0), X_test.sum().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7ffe7e-095a-47fe-baea-541573ed7813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for model_name, model in zip(model_names, lst_models):\n",
    "#     best_model = torch.load( f\"model/{model_name}_best_model.pth\")\n",
    "#     model.load_state_dict(best_model)\n",
    "#     model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821ce1b4-a503-4976-a4d7-7b4829e865a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c9be7b-b759-4b19-83a6-a397c3e7c00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# import umap\n",
    "\n",
    "def plot_latent_space(latent, method='tsne', colorby = None, cbar_label=None):\n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "    elif method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method\")\n",
    "\n",
    "    reduced = reducer.fit_transform(latent)\n",
    "    \n",
    "    scatter = plt.scatter(reduced[:, 0], reduced[:, 1], \n",
    "                          c=colorby, cmap='viridis',\n",
    "                          s=10, alpha=0.6, edgecolors='w', linewidths=0.5)\n",
    "    plt.colorbar(scatter, label=cbar_label)\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c3292e-7aa0-4d56-bf88-841684d5f392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "non_zero_counts = (X_test != 0).sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(15, 4 * len(model_names)))\n",
    "i = 1\n",
    "for model_name, latent_vectors in dct_latent_vectors.items():\n",
    "    plt.subplot(len(dct_latent_vectors), 2, i)\n",
    "    plot_latent_space(latent_vectors, method='pca', colorby=non_zero_counts, cbar_label='Number of non-zero Features')\n",
    "    plt.title(f'{model_name} - PCA')\n",
    "    i += 1\n",
    "    plt.subplot(len(dct_latent_vectors), 2, i)\n",
    "    plot_latent_space(latent_vectors, method='tsne', colorby=non_zero_counts, cbar_label='Number of non-zero Features')\n",
    "    plt.title(f'{model_name} - TSNE')\n",
    "    i+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6286072b-cafe-487f-ae2d-04389a2303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4*len(model_names)))\n",
    "i = 1\n",
    "for model_name, latent_vectors in dct_latent_vectors.items():\n",
    "    plt.subplot(len(dct_latent_vectors), 2, i)\n",
    "    plot_latent_space(latent_vectors, method='pca', colorby=y_test, cbar_label='Age')\n",
    "    plt.title(f'{model_name} - PCA')\n",
    "    i += 1\n",
    "    plt.subplot(len(dct_latent_vectors), 2, i)\n",
    "    plot_latent_space(latent_vectors, method='tsne', colorby=y_test, cbar_label='Age')\n",
    "    plt.title(f'{model_name} - TSNE')\n",
    "    i+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e2b3a9-6941-443d-bdd8-151aaefe1e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"../data/metadata.txt\", sep='\\t', header=0, index_col=0)\n",
    "df_meta = df_meta.iloc[test_idx, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabe7825-29da-41cb-b537-c55b53ad3d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_meta['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caffa309-76b1-4051-a3b1-3320140e3cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9f5dbe-59ef-4b3c-8d8e-22841128ddf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explainer = shap.KernelExplainer(model, torch.tensor(X_test, dtype=torch.float32))\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# # Plotting SHAP values\n",
    "# shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac8229f-8550-4c58-981d-8b8762a5577d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for model_name, model in zip(model_names, models):\n",
    "#   explainer = shap.KernelExplainer(model.encoder.predict, input_data)\n",
    "#   shap_values = explainer.shap_values(input_data)\n",
    "\n",
    "# # Plotting SHAP values\n",
    "# shap.summary_plot(shap_values, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fafa573-a4cc-4a5d-896b-c83473c096ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e97441d-fbd8-4d3d-9433-c23909b66761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abfe91a1-26b6-44a0-a8ef-3730dc6d8576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !load_ext tensorboard\n",
    "# !tensorboard --logdir $log_dir -- port 6006"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_cMD_Autoencoders + xgb regressor for Age Prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
