{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32656a72-c372-41c1-a4d6-040a2e7f3cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae2398a-dc2b-42c9-84c4-52d08d73ad56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    GPT2Config, \n",
    "    GPT2LMHeadModel, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from utils_gpt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b20003-08f7-4d3c-abfd-d5a08e638a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Force CUDA initialization at the start\n",
    "if torch.cuda.is_available():\n",
    "    torch.zeros(1).cuda()  # Triggers CUDA context init\n",
    "    torch.cuda.manual_seed_all(42)  # Now safe to set seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb8ad20-a756-45d3-9d47-2d64a3483ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# customized tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099c8660-fd83-4603-93d8-475eddf76a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, token_to_id, id_to_token):\n",
    "        self.token_to_id = token_to_id\n",
    "        self.id_to_token = id_to_token\n",
    "        self.pad_token_id = token_to_id[\"<pad>\"]\n",
    "        self.bos_token_id = token_to_id[\"<s>\"]\n",
    "        self.eos_token_id = token_to_id[\"</s>\"]\n",
    "        self.unk_token_id = token_to_id[\"<unk>\"]\n",
    "        self.vocab_size = len(token_to_id)\n",
    "        \n",
    "        # Special token attributes expected by HF transformers\n",
    "        self.all_special_ids = [self.pad_token_id, self.bos_token_id, self.eos_token_id, self.unk_token_id]\n",
    "        self.model_max_length = MAX_SEQ_LENGTH\n",
    "\n",
    "        # Special token properties that HF expects\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.bos_token = \"<s>\"\n",
    "        self.eos_token = \"</s>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        \n",
    "        # Map IDs to special tokens\n",
    "        self.special_ids_to_tokens = {\n",
    "            self.pad_token_id: self.pad_token,\n",
    "            self.bos_token_id: self.bos_token,\n",
    "            self.eos_token_id: self.eos_token,\n",
    "            self.unk_token_id: self.unk_token\n",
    "        }\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Split text into tokens\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            return text.split()\n",
    "        return text\n",
    "\n",
    "    def __call__(self, sequence, max_length=None, padding=False, truncation=False, return_tensors=None):\n",
    "        \"\"\"Make the tokenizer callable like HF tokenizers\"\"\"\n",
    "        return self.encode(sequence, max_length, padding, truncation, return_tensors)\n",
    "    \n",
    "\n",
    "    def encode(self, sequence, max_length=None, padding=False, truncation=False, return_tensors=None):\n",
    "        \"\"\"Convert a sequence or batch of sequences to token IDs\"\"\"\n",
    "        # Check if it's a batch (list of strings)\n",
    "        if isinstance(sequence, list) and all(isinstance(s, str) for s in sequence):\n",
    "            # Process batch\n",
    "            batch_ids = []\n",
    "            batch_attention_masks = []\n",
    "            \n",
    "            # First pass: tokenize and truncate\n",
    "            for seq in sequence:\n",
    "                seq_tokens = seq.split()\n",
    "                \n",
    "                # Convert tokens to ids\n",
    "                ids = [self.token_to_id.get(token, self.unk_token_id) for token in seq_tokens]\n",
    "                \n",
    "                # Apply truncation if needed\n",
    "                if truncation and max_length and len(ids) > max_length:\n",
    "                    ids = ids[:max_length]\n",
    "                \n",
    "                batch_ids.append(ids)\n",
    "            \n",
    "            # Determine the padding length\n",
    "            if padding:\n",
    "                if max_length is None:\n",
    "                    # Pad to the longest sequence in the batch\n",
    "                    max_length = max(len(ids) for ids in batch_ids)\n",
    "                \n",
    "                # Second pass: pad all sequences to max_length\n",
    "                for i, ids in enumerate(batch_ids):\n",
    "                    original_length = len(ids)\n",
    "                    attention_mask = [1] * original_length\n",
    "                    \n",
    "                    padding_length = max_length - original_length\n",
    "                    if padding_length > 0:\n",
    "                        ids = ids + [self.pad_token_id] * padding_length\n",
    "                        attention_mask = attention_mask + [0] * padding_length\n",
    "                    \n",
    "                    batch_ids[i] = ids\n",
    "                    batch_attention_masks.append(attention_mask)\n",
    "            else:\n",
    "                # No padding - just create attention masks\n",
    "                for ids in batch_ids:\n",
    "                    attention_mask = [1] * len(ids)\n",
    "                    batch_attention_masks.append(attention_mask)\n",
    "            \n",
    "            # Return tensors if requested\n",
    "            if return_tensors == \"pt\":\n",
    "                return {\n",
    "                    \"input_ids\": torch.tensor(batch_ids),\n",
    "                    \"attention_mask\": torch.tensor(batch_attention_masks)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"input_ids\": batch_ids,\n",
    "                    \"attention_mask\": batch_attention_masks\n",
    "                }\n",
    "        else:\n",
    "            # Single sequence processing (original code)\n",
    "            if isinstance(sequence, str):\n",
    "                sequence = sequence.split()\n",
    "                \n",
    "            # Convert tokens to ids\n",
    "            ids = [self.token_to_id.get(token, self.unk_token_id) for token in sequence]\n",
    "            \n",
    "            # Apply truncation if needed\n",
    "            if truncation and max_length and len(ids) > max_length:\n",
    "                ids = ids[:max_length]\n",
    "                \n",
    "            # Apply padding if needed\n",
    "            attention_mask = [1] * len(ids)\n",
    "            if padding and max_length:\n",
    "                padding_length = max_length - len(ids)\n",
    "                ids = ids + [self.pad_token_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "            \n",
    "            # Return tensors if requested\n",
    "            if return_tensors == \"pt\":\n",
    "                return {\n",
    "                    \"input_ids\": torch.tensor([ids]),\n",
    "                    \"attention_mask\": torch.tensor([attention_mask])\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"input_ids\": ids,\n",
    "                    \"attention_mask\": attention_mask\n",
    "                }\n",
    "    \n",
    "    def decode(self, ids, skip_special_tokens=False):\n",
    "        \"\"\"Convert token IDs back to a sequence\"\"\"\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "            \n",
    "        tokens = []\n",
    "        for id in ids:\n",
    "            if skip_special_tokens and id in self.all_special_ids:\n",
    "                continue\n",
    "            tokens.append(self.id_to_token.get(id, \"<unk>\"))\n",
    "            \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def save_pretrained(self, output_dir):\n",
    "        \"\"\"Save tokenizer to disk\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the vocabulary\n",
    "        with open(os.path.join(output_dir, \"vocab.json\"), \"w\") as f:\n",
    "            # Convert int keys to strings for JSON serialization\n",
    "            token_to_id_serializable = {k: v for k, v in self.token_to_id.items()}\n",
    "            id_to_token_serializable = {str(k): v for k, v in self.id_to_token.items()}\n",
    "            json.dump({\n",
    "                \"token_to_id\": token_to_id_serializable,\n",
    "                \"id_to_token\": id_to_token_serializable\n",
    "            }, f)\n",
    "            \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, input_dir):\n",
    "        \"\"\"Load tokenizer from disk\"\"\"\n",
    "        with open(os.path.join(input_dir, \"vocab.json\"), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            token_to_id = data[\"token_to_id\"]\n",
    "            # Convert string keys back to integers\n",
    "            id_to_token = {int(k): v for k, v in data[\"id_to_token\"].items()}\n",
    "            \n",
    "        return cls(token_to_id, id_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4811f73-3f07-4919-ad4b-1bb2ad01f9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 150\n",
    "tokenizer_path = \"models/simple_taxa_tokenizer\"\n",
    "\n",
    "# load mapping dictionaries\n",
    "import json\n",
    "with open('../../data/token_to_id.json', 'r') as f:\n",
    "    token_to_id = json.load(f)\n",
    "id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = {\"<pad>\": 0, \"<s>\":  1, \n",
    "                    \"</s>\":  2, \"<unk>\": 3}\n",
    "token_to_id.update(special_tokens)\n",
    "id_to_token.update({v: k for k, v in special_tokens.items()})\n",
    "\n",
    "tokenizer = SimpleTokenizer(token_to_id, id_to_token)\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8f2905-0f89-428b-9a3c-722ecc17f1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_TEXTS = [\n",
    "        # \"This is a simple test sentence.\",\n",
    "        # \"Let's test some domain-specific content that your model might see.\",\n",
    "        \"Roseburia Ruminococcus Streptococcus Dorea Faecalibacterium Anaerostipes Bifidobacterium Blautia Anaerobutyricum Agathobaculum Collinsella Klebsiella Fusicatenibacter Bacteroides Eubacterium Gemmiger Adlercreutzia Phocaeicola Alistipes Barnesiella Firmicutes\",\n",
    "        \"Ruminococcus Phocaeicola Bacteroides Faecalibacterium Eubacterium Roseburia Alistipes\"\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cb40f4-caa7-4563-860d-7178bd42e2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(SAMPLE_TEXTS, padding=True, truncation=True,\n",
    "                  return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "598b7750-7559-4586-86f7-63d28583a416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# dataset and dataloader classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a072785-2f1a-40e1-9cc7-c51c3d4d60c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# custom dataset class\n",
    "class TaxaSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: List of sequences, where each sequence is a list of string tokens\n",
    "            tokenizer: Simple tokenizer instance\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.inputs = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            encoded = tokenizer.encode(\n",
    "                seq,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            self.inputs.append(encoded)    \n",
    "\n",
    "            # self.inputs.append({\n",
    "            #     \"input_ids\": encoded[\"input_ids\"],\n",
    "            #     \"attention_mask\": encoded[\"attention_mask\"]\n",
    "            # })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get raw tensors\n",
    "        input_ids = self.inputs[idx][\"input_ids\"]\n",
    "        attention_mask = self.inputs[idx][\"attention_mask\"]\n",
    "        \n",
    "        # Ensure both tensors are 1D (flatten if needed)\n",
    "        if input_ids.dim() > 1:\n",
    "            input_ids = input_ids.view(-1)\n",
    "        if attention_mask.dim() > 1:\n",
    "            attention_mask = attention_mask.view(-1)\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31a3fae-cd3b-4835-9d35-59519b86ae71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# configure and initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f39bf7-bdcc-4197-9d5b-3c35c26e6c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "model chosen: GPT-2 Small (gpt2) \n",
    "\n",
    "- It's autoregressive, suited for generation task (taxa completion)\n",
    "- practical to extract embeddings for downstream supervised learning tasks\n",
    "- It handles variable sequence lengths well\n",
    "- With 355 unique values, no worry for vocabulary size issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87770240-ef55-4742-b8c4-d241753a2b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f7e3cf-bd0a-4ae9-8311-6f99a99ff2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_gpt2_model(vocab_size):\n",
    "    \"\"\"Create a GPT-2 model with custom vocab size\"\"\"\n",
    "    config = GPT2Config(\n",
    "        vocab_size=vocab_size,\n",
    "        n_positions=MAX_SEQ_LENGTH,\n",
    "        n_embd=64,  # Smaller embedding size\n",
    "        n_layer=6,   # Fewer layers for faster training\n",
    "        n_head=4,    # Fewer attention heads\n",
    "\n",
    "        bos_token_id=1,  # <s>\n",
    "        eos_token_id=2,  # </s>\n",
    "        pad_token_id=0,   # <pad>\n",
    "        unk_token_id =3,  # <unk>\n",
    "\n",
    "        attn_pdrop = 0.1,  # Attention dropout\n",
    "        embd_pdrop = 0.1,  # Embedding dropout\n",
    "        resid_pdrop = 0.1,  # Residual dropout\n",
    "\n",
    "        # optimizations for small dataset\n",
    "        layer_norm_epsilon=1e-05,\n",
    "        initializer_range=0.02,\n",
    "        use_cache=False  # Disable during training\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel(config)\n",
    "    return model\n",
    "\n",
    "model = create_gpt2_model(tokenizer.vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ae470e-752d-41b8-9eeb-64fc232c09bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed33a34f-cfd1-4384-8ae6-cdff9f92874c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a SimpleDataCollator class to replace the HF DataCollatorForLanguageModeling, to Properly handles padding and creates proper language modeling labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eff66cd-1e8a-4d6b-bba2-a99e7426fa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimpleDataCollator:\n",
    "    \"\"\"Simple data collator for language modeling\"\"\"\n",
    "    def __init__(self, tokenizer, mlm=False, max_seq_length=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        # Ensure consistent tensor shapes\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
    "        \n",
    "        # Get max length\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        if self.max_seq_length:\n",
    "            max_len = min(max_len, self.max_seq_length)\n",
    "        \n",
    "        \n",
    "        # Pad all tensors to max length\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            # Padding needed\n",
    "            padding_len = max_len - len(ids)\n",
    "            \n",
    "            if padding_len > 0:\n",
    "                # Pad with pad_token_id\n",
    "                padded_ids = torch.cat([\n",
    "                    ids, \n",
    "                    torch.full((padding_len,), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "                ])\n",
    "                padded_mask = torch.cat([\n",
    "                    mask,\n",
    "                    torch.zeros(padding_len, dtype=torch.long)\n",
    "                ])\n",
    "            else:\n",
    "                padded_ids = ids\n",
    "                padded_mask = mask\n",
    "                \n",
    "            padded_input_ids.append(padded_ids)\n",
    "            padded_attention_mask.append(padded_mask)\n",
    "        \n",
    "        # Stack into batches\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack(padded_input_ids),\n",
    "            \"attention_mask\": torch.stack(padded_attention_mask)\n",
    "        }\n",
    "        \n",
    "        # For causal language modeling\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        # Mark padding as -100 to ignore in loss calculation\n",
    "        labels[batch[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        def debug_batch(batch):\n",
    "            print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "            print(\"Attention mask shape:\", batch[\"attention_mask\"].shape)\n",
    "            print(\"Labels shape:\", batch[\"labels\"].shape)\n",
    "            # Check if all tensors are on the same device\n",
    "            print(\"Input IDs device:\", batch[\"input_ids\"].device)\n",
    "            print(\"Attention mask device:\", batch[\"attention_mask\"].device)\n",
    "            print(\"Labels device:\", batch[\"labels\"].device)\n",
    "            # Check for any NaN values\n",
    "            print(\"Any NaN in input_ids:\", torch.isnan(batch[\"input_ids\"]).any())\n",
    "            print(\"Any NaN in attention_mask:\", torch.isnan(batch[\"attention_mask\"]).any())\n",
    "            print(\"Any NaN in labels:\", torch.isnan(batch[\"labels\"]).any())\n",
    "            # Print some values\n",
    "            print(\"First sequence input_ids:\", batch[\"input_ids\"][0][:10])\n",
    "            print(\"First sequence attention_mask:\", batch[\"attention_mask\"][0][:10])\n",
    "            print(\"First sequence labels:\", batch[\"labels\"][0][:10])\n",
    "\n",
    "        # Use this before model forward pass\n",
    "        # debug_batch(batch)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15bef3b3-1a43-488c-a11e-7497c98be1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520f9326-0d21-4198-a587-a0b0b5b11345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load sequences\n",
    "with open('../../data/taxa_sequences.txt', 'r') as file:\n",
    "    sequences = file.readlines()\n",
    "\n",
    "# Split sequences into train and test sets\n",
    "train_sequences, test_sequences = train_test_split(sequences, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dd9be3-cbe9-4fe7-84b2-e4832d39b784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ece300f-08b9-476b-8f45-f32fcf2bd523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11506ee0-7df6-4750-bc5d-188fcb46fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir=\"./gpt2_taxa_seq_model\"\n",
    "train_dataset = TaxaSequenceDataset(train_sequences, tokenizer)\n",
    "eval_dataset = TaxaSequenceDataset(test_sequences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73efad10-81d5-449b-a601-32c7ae97e730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_diversity_metrics(model, tokenizer, num_samples=100):\n",
    "    \"\"\"Calculate and log various diversity metrics to MLflow\"\"\"\n",
    "    \"\"\"help to monitor if the model is learning diverse patterns or just memorizing training data.\"\"\"\n",
    "    generated_sequences = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(model.device)\n",
    "            \n",
    "            generated = model.generate(\n",
    "                input_ids,\n",
    "                max_length=25,\n",
    "                do_sample=True,\n",
    "                temperature=1.0,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode for sequence diversity\n",
    "            sequence = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "            generated_sequences.append(sequence)\n",
    "            \n",
    "            # Keep token IDs for token diversity\n",
    "            all_tokens.extend(generated[0].tolist())\n",
    "    \n",
    "    # Sequence-level diversity\n",
    "    unique_sequences = len(set(generated_sequences))\n",
    "    sequence_diversity = unique_sequences / len(generated_sequences)\n",
    "    \n",
    "    # Token-level diversity\n",
    "    unique_tokens = len(set(all_tokens))\n",
    "    vocab_usage = unique_tokens / tokenizer.vocab_size\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"sequence_diversity\", sequence_diversity)\n",
    "    mlflow.log_metric(\"unique_sequences\", unique_sequences)\n",
    "    mlflow.log_metric(\"vocab_usage_ratio\", vocab_usage)\n",
    "    \n",
    "    print(f\"Diversity Metrics:\")\n",
    "    print(f\"  Sequence diversity: {sequence_diversity:.3f}\")\n",
    "    print(f\"  Unique sequences: {unique_sequences}/{num_samples}\")\n",
    "    print(f\"  Vocabulary usage: {vocab_usage:.3f} ({unique_tokens}/{tokenizer.vocab_size} tokens)\")\n",
    "    \n",
    "    model.train()\n",
    "    return sequence_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86135d37-4f9f-4037-9006-afdd62a71cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from transformers import Trainer, TrainerCallback\n",
    "from transformers.integrations import MLflowCallback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class TaxaModelCallback(TrainerCallback):\n",
    "    \"\"\"Simplified callback for Databricks with MLflow logging\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, generation_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generation_length = generation_length\n",
    "        self.epoch_samples = []\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"Generate samples and log to MLflow\"\"\"\n",
    "        if state.global_step > 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Generate sample sequence\n",
    "            input_ids = torch.tensor([[self.tokenizer.bos_token_id]]).to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=self.generation_length,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Log to MLflow\n",
    "            mlflow.log_text(generated_text, f\"samples/epoch_{state.epoch}_sample.txt\")\n",
    "            mlflow.log_metric(\"epoch\", state.epoch, step=state.global_step)\n",
    "            \n",
    "            # Calculate and log perplexity if eval loss exists\n",
    "            if hasattr(state, 'log_history') and state.log_history:\n",
    "                last_log = state.log_history[-1]\n",
    "                if 'eval_loss' in last_log:\n",
    "                    perplexity = np.exp(last_log['eval_loss'])\n",
    "                    mlflow.log_metric(\"perplexity\", perplexity, step=state.global_step)\n",
    "            \n",
    "            # Display in notebook\n",
    "            print(f\"\\nEpoch {state.epoch}: Generated sample:\")\n",
    "            print(generated_text[:100] + \"...\" if len(generated_text) > 100 else generated_text)\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "\n",
    "class SimpleTaxaTrainer(Trainer):\n",
    "    \"\"\"Minimal custom trainer for taxa data\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"Standard loss computation\"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Log batch metrics to MLflow\n",
    "        if self.state.global_step % 100 == 0:\n",
    "            mlflow.log_metric(\"batch_loss\", loss.item(), step=self.state.global_step)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5470251d-ed82-4452-b299-b9c492a898da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, train_sequences, eval_sequences=None, \n",
    "                output_dir=\"./gpt2_taxa_seq_model\", num_epochs=200):\n",
    "    \"\"\"\n",
    "    Train GPT-2 model on taxa sequences - simplified for Databricks\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = TaxaSequenceDataset(train_sequences, tokenizer)\n",
    "    eval_dataset = None\n",
    "    if eval_sequences:\n",
    "        eval_dataset = TaxaSequenceDataset(eval_sequences, tokenizer)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = SimpleDataCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        max_seq_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if eval_dataset else False,\n",
    "        metric_for_best_model=\"eval_loss\" if eval_dataset else None,\n",
    "        greater_is_better=False,\n",
    "        learning_rate=5e-4,\n",
    "        warmup_steps=1000,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=100,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    # Only add custom callbacks - Databricks handles MLflow automatically\n",
    "    callbacks = []\n",
    "    \n",
    "    # Add taxa-specific callback if you want sample generation\n",
    "    if eval_dataset:\n",
    "        callbacks.append(TaxaModelCallback(tokenizer=tokenizer, generation_length=50))\n",
    "        callbacks.append(EarlyStoppingCallback(\n",
    "            early_stopping_patience=20,\n",
    "            early_stopping_threshold=0.001\n",
    "        ))\n",
    "    \n",
    "    # Initialize trainer - Databricks will handle MLflow logging automatically\n",
    "    trainer = Trainer(  # Use standard Trainer\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        callbacks=callbacks if callbacks else None\n",
    "    )\n",
    "    \n",
    "    # Train the model - MLflow tracking happens automatically\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the trained model\n",
    "    model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Generate a few final samples to verify quality\n",
    "    model.eval()\n",
    "    print(\"\\n=== Final Generated Samples ===\")\n",
    "    for i in range(3):\n",
    "        input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                input_ids,\n",
    "                max_length=50,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "            )\n",
    "        sequence = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        print(f\"Sample {i+1}: {sequence}\")\n",
    "    \n",
    "    print(f\"Training complete! Model saved to {output_dir}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2adba993-3d35-49c8-9513-80228758f540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd77329f-3844-44c8-a6cd-1bb1a3ab38d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure MLflow autologging is enabled\n",
    "\n",
    "model, tokenizer = train_model(model, tokenizer, train_sequences, test_sequences, \"./gpt2_taxa_seq_model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f41926-ebec-496c-ac8b-b75bac827c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# evaluate on sequence completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c2fe6c-4656-4f6d-991e-75ef39d9d000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_taxa_seq_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac08f0a-d51e-4f44-87dd-f48d1233b8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9018dd4-12dd-4d2e-81f2-177e8b261dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "\n",
    "# First, generate and save sequences once\n",
    "def generate_and_save_sequences(model, test_sequences, save_path='generated_sequences.pkl', \n",
    "                               prefix_ratio=0.3, num_samples=None):\n",
    "    \"\"\"Generate sequences once and save them for later metric calculation\"\"\"\n",
    "    generated_data = []\n",
    "    \n",
    "    # Use subset if specified\n",
    "    sequences_to_process = test_sequences[:num_samples] if num_samples else test_sequences\n",
    "    \n",
    "    for idx, test_seq in enumerate(sequences_to_process):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing sequence {idx}/{len(sequences_to_process)}\")\n",
    "        \n",
    "        seq_tokens = test_seq.split()\n",
    "        prefix_len = max(3, int(len(seq_tokens) * prefix_ratio))  # Use at least 3 tokens as prefix\n",
    "        prefix_tokens = seq_tokens[:prefix_len]\n",
    "        target_tokens = seq_tokens[prefix_len:]\n",
    "\n",
    "        prefix = \" \".join(prefix_tokens)\n",
    "        target = \" \".join(target_tokens)\n",
    "        \n",
    "        # Generate completion using the prefix\n",
    "        generated = generate_from_seed(model, tokenizer, prefix, max_length=len(seq_tokens) + 5)\n",
    "        \n",
    "        # Extract the generated continuation (after the prefix)\n",
    "        generated_completion = \" \".join(generated.split()[prefix_len:])\n",
    "\n",
    "\n",
    "\n",
    "        # # Generate sequence\n",
    "        # generated_tokens = model.generate(prefix_tokens, max_new_tokens=len(target_tokens))\n",
    "        \n",
    "        # Save all relevant data\n",
    "        generated_data.append({\n",
    "            'prefix_tokens': prefix_tokens,\n",
    "            'target_tokens': target_tokens,\n",
    "            'generated_tokens': generated_completion.split()\n",
    "        })\n",
    "    \n",
    "    # Save to file\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(generated_data, f)\n",
    "    \n",
    "    print(f\"Saved {len(generated_data)} generated sequences to {save_path}\")\n",
    "    return generated_data\n",
    "\n",
    "# Then calculate metrics from saved data\n",
    "def calculate_metrics_from_saved(save_path='generated_sequences.pkl'):\n",
    "    \"\"\"Calculate multiple metrics from saved generated sequences\"\"\"\n",
    "    \n",
    "    # Load saved data\n",
    "    with open(save_path, 'rb') as f:\n",
    "        generated_data = pickle.load(f)\n",
    "    \n",
    "    # Initialize metric lists\n",
    "    jaccard_scores = []\n",
    "    f1_scores = []\n",
    "    weighted_jaccard_scores = []\n",
    "    top_k_scores = []\n",
    "    normalized_edit_scores = []\n",
    "    \n",
    "    # Your original accuracy for comparison\n",
    "    accuracies = []\n",
    "    \n",
    "    for data in generated_data:\n",
    "        target_tokens = data['target_tokens']\n",
    "        generated_tokens = data['generated_tokens']\n",
    "        \n",
    "        # accuracy calculation (for comparison)\n",
    "        matches = sum(1 for t, g in zip(target_tokens, generated_tokens) if t == g)\n",
    "        accuracy = matches / len(target_tokens) if target_tokens else 1.0\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Set-based metrics\n",
    "        set_target = set(target_tokens)\n",
    "        set_generated = set(generated_tokens)\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        jaccard = len(set_target & set_generated) / len(set_target | set_generated) if set_target | set_generated else 0\n",
    "        jaccard_scores.append(jaccard)\n",
    "        \n",
    "        # F1 score\n",
    "        precision = len(set_target & set_generated) / len(set_generated) if set_generated else 0\n",
    "        recall = len(set_target & set_generated) / len(set_target) if set_target else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # Position-weighted Jaccard\n",
    "        def calculate_weighted_jaccard(target, generated, position_weight=0.9):\n",
    "            score = 0\n",
    "            for i, taxon in enumerate(generated):\n",
    "                if taxon in target:\n",
    "                    target_pos = target.index(taxon)\n",
    "                    weight = position_weight ** abs(i - target_pos)\n",
    "                    score += weight\n",
    "            return score / len(target) if target else 0\n",
    "        \n",
    "        weighted_jaccard = calculate_weighted_jaccard(target_tokens, generated_tokens)\n",
    "        weighted_jaccard_scores.append(weighted_jaccard)\n",
    "        \n",
    "        # Top-k accuracy (top-10)\n",
    "        k = min(10, len(target_tokens))  # Handle cases where sequence is shorter than 10\n",
    "        top_k_accuracy = len(set(target_tokens[:k]) & set(generated_tokens[:k])) / k if k > 0 else 0\n",
    "        top_k_scores.append(top_k_accuracy)\n",
    "        \n",
    "        # Normalized edit distance\n",
    "        edit_dist = distance(target_tokens, generated_tokens)\n",
    "        max_len = max(len(target_tokens), len(generated_tokens))\n",
    "        normalized_edit = 1 - (edit_dist / max_len) if max_len > 0 else 1\n",
    "        normalized_edit_scores.append(normalized_edit)\n",
    "    \n",
    "    # Calculate averages\n",
    "    results = {\n",
    "        'accuracy': np.mean(accuracies),\n",
    "        'jaccard_similarity': np.mean(jaccard_scores),\n",
    "        'f1_score': np.mean(f1_scores),\n",
    "        'weighted_jaccard': np.mean(weighted_jaccard_scores),\n",
    "        'top_k_accuracy': np.mean(top_k_scores),\n",
    "        'normalized_edit_distance': np.mean(normalized_edit_scores),\n",
    "        # Also include standard deviations\n",
    "        'accuracy_std': np.std(accuracies),\n",
    "        'jaccard_similarity_std': np.std(jaccard_scores),\n",
    "        'f1_score_std': np.std(f1_scores),\n",
    "        'weighted_jaccard_std': np.std(weighted_jaccard_scores),\n",
    "        'top_k_accuracy_std': np.std(top_k_scores),\n",
    "        'normalized_edit_distance_std': np.std(normalized_edit_scores)\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d363ec-a857-4311-9766-5dc31dd0c5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate once and save\n",
    "generated_data = generate_and_save_sequences(\n",
    "    model, \n",
    "    test_sequences, \n",
    "    save_path='generated_sequences_test.pkl',\n",
    "    num_samples=500\n",
    ")\n",
    "\n",
    "# Calculate metrics from saved data\n",
    "metrics = calculate_metrics_from_saved('generated_sequences_test.pkl')\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f} ± {metrics['accuracy_std']:.4f}\")\n",
    "print(f\"Jaccard Similarity: {metrics['jaccard_similarity']:.4f} ± {metrics['jaccard_similarity_std']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f} ± {metrics['f1_score_std']:.4f}\")\n",
    "print(f\"Weighted Jaccard: {metrics['weighted_jaccard']:.4f} ± {metrics['weighted_jaccard_std']:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {metrics['top_k_accuracy']:.4f} ± {metrics['top_k_accuracy_std']:.4f}\")\n",
    "print(f\"Normalized Edit Distance: {metrics['normalized_edit_distance']:.4f} ± {metrics['normalized_edit_distance_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d509ad0-8aec-42cd-a1a1-5ac280fdddef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matthieu : distance entre les vrai et predicted order  à tester . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa9d1d16-16a9-4550-b627-75be583bc926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matthieu: can we integrate aussi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7adb7262-8f86-4696-969c-2ad909e01f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def evaluate_sequence_completion(model, tokenizer, test_sequences, prefix_ratio=0.5):\n",
    "#     model.eval()\n",
    "#     accuracies = []\n",
    "    \n",
    "#     for sequence in test_sequences:\n",
    "#         # Split the sequence into prefix and target\n",
    "#         tokens = sequence.split()\n",
    "#         prefix_len = max(3, int(len(tokens) * prefix_ratio))  # Use at least 3 tokens as prefix\n",
    "        \n",
    "#         prefix = \" \".join(tokens[:prefix_len])\n",
    "#         target = \" \".join(tokens[prefix_len:])\n",
    "        \n",
    "#         # Generate completion using the prefix\n",
    "#         generated = generate_from_seed(model, tokenizer, prefix, max_length=len(tokens) + 5)\n",
    "        \n",
    "#         # Extract the generated continuation (after the prefix)\n",
    "#         generated_completion = \" \".join(generated.split()[prefix_len:])\n",
    "        \n",
    "#         # Calculate accuracy (exact match between tokens)\n",
    "#         target_tokens = target.split()\n",
    "#         generated_tokens = generated_completion.split()[:len(target_tokens)]  # Truncate to target length\n",
    "        \n",
    "#         # If generated is shorter, pad with dummy values that won't match\n",
    "#         if len(generated_tokens) < len(target_tokens):\n",
    "#             generated_tokens.extend([\"DUMMY\"] * (len(target_tokens) - len(generated_tokens)))\n",
    "        \n",
    "#         # Calculate token-level accuracy\n",
    "#         matches = sum(1 for t, g in zip(target_tokens, generated_tokens) if t == g)\n",
    "#         accuracy = matches / len(target_tokens) if target_tokens else 1.0\n",
    "#         accuracies.append(accuracy)\n",
    "        \n",
    "#     return {\n",
    "#         \"mean_accuracy\": np.mean(accuracies),\n",
    "#         \"individual_accuracies\": accuracies\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58fea9c-8f1e-46b8-9ee6-711f6c69c084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completion_results = evaluate_sequence_completion(model, tokenizer, test_sequences)\n",
    "print(f\"Mean sequence completion accuracy: {completion_results['mean_accuracy']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76a9354-3829-4d9b-97db-dd5f3849efd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380e8c61-2f70-40ea-b8fd-f0a2464502b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example of sequence completion\n",
    "example_idx = np.random.randint(0, len(test_sequences))\n",
    "example_sequence = test_sequences[example_idx]\n",
    "tokens = example_sequence.split()\n",
    "prefix_len = max(3, int(len(tokens) * 0.5))\n",
    "prefix = \" \".join(tokens[:prefix_len])\n",
    "\n",
    "print(\"\\nExample sequence completion:\")\n",
    "print(f\"Prefix: {prefix}\")\n",
    "print(f\"Original completion: {' '.join(tokens[prefix_len:])}\")\n",
    "generated = generate_from_seed(model, tokenizer, prefix, max_length=len(tokens) + 5)\n",
    "print(f\"Model completion: {' '.join(generated.split()[prefix_len:])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72150d7b-6065-4a11-b4ea-725e0f2ee671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(tokens), len(generated.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2489a6-1211-4fed-8d68-e221ff0e9977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "intersection = set(tokens).intersection(set(generated.split()))\n",
    "percentage_intersection = (len(intersection) / len(tokens)) * 100\n",
    "percentage_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3378b117-b44a-4991-a741-0349b0b3d1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd3b9755-0ee1-49b9-ab30-7d9288287544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8e80780-8e26-41b2-a9a9-febfd28bb7f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "matthieu: extract embedding ! and compare effect diet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e223e4f6-0b2f-4979-b5b1-107950956d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract embeddings for a few test sequences\n",
    "print(\"\\nExtracting embeddings for supervised learning...\")\n",
    "sample_sequences = test_sequences[:5]  # Just use a few sequences for demonstration\n",
    "embeddings = extract_embeddings(model, tokenizer, sample_sequences)\n",
    "print(f\"Extracted embeddings shape: {embeddings.shape}\")\n",
    "print(\"These embeddings can now be used for classification or regression tasks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e65cf5c2-61a1-443b-8ba0-8a121e1f980a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "https://adb-7744086575777980.0.azuredatabricks.net/explore/data/onesource_eu_dev_rni/onebiome/mpa_2?o=7744086575777980"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_curatedMetagenomic_Pretraining_GPT2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
