{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32656a72-c372-41c1-a4d6-040a2e7f3cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95ee7555-e0c1-49e5-af5b-3d5dc287d30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: First, isolate the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07979322-d6d4-41d9-87f9-73e9ee1c54eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('gpt2_pretraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a998ddee-9a22-476d-8498-542f9789b205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    logger.info(\"Starting tokenization...\")\n",
    "    TOKENIZER_PATH = \"models/simple_taxa_tokenizer\"  # Set to None to use standard GPT-2\n",
    "    SAMPLE_TEXTS = [\n",
    "        \"This is a simple test sentence.\",\n",
    "        \"Let's test some domain-specific content that your model might see.\",\n",
    "        \"Ruminococcus Phocaeicola Bacteroides Faecalibacterium Eubacterium Roseburia Alistipes\"\n",
    "    ]    \n",
    "    success, message = test_tokenizer(TOKENIZER_PATH, SAMPLE_TEXTS)\n",
    "    logger.info(f\"Test result: {message}\")\n",
    "    logger.info(\"Tokenization complete\")\n",
    "\n",
    "\n",
    "    logger.info(\"Preparing dataset...\")\n",
    "    DATA_PATH = \"../../data/taxa_sequences.txt\"\n",
    "    TOKENIZER_PATH = \"gpt2\"  # Update this\n",
    "    success, message = test_data_loading(DATA_PATH, TOKENIZER_PATH)\n",
    "    logger.info(f\"Test result: {message}\")\n",
    "    logger.info(\"Dataset preparation complete\")\n",
    "    \n",
    "\n",
    "    logger.info(\"Initializing model...\")\n",
    "    TOKENIZER_PATH = \"gpt2\"  # Update this\n",
    "    \n",
    "    # Custom config if needed\n",
    "    custom_config = GPT2Config(\n",
    "        vocab_size=50257,  # Update with your tokenizer's vocab size\n",
    "        n_positions=512,\n",
    "        n_ctx=512,\n",
    "        n_embd=768,\n",
    "        n_layer=6,  # Smaller than standard GPT-2 for faster testing\n",
    "        n_head=12,\n",
    "    )    \n",
    "    success, message = test_model(TOKENIZER_PATH, config=custom_config)\n",
    "    logger.info(f\"Test result: {message}\")\n",
    "    logger.info(\"Model initialization complete\")\n",
    "    \n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    TOKENIZER_PATH = \"gpt2\"  # Update this\n",
    "    success, message = test_training(TOKENIZER_PATH)\n",
    "    logger.info(f\"Test result: {message}\")\n",
    "    logger.info(\"Training complete\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred: {str(e)}\")\n",
    "    logger.error(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d5999f6-ff20-4bd8-a06a-c5055e6eb44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# step 2: test the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d162e7f8-85c2-40e7-88a5-3860f15e47a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import GPT2TokenizerFast\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"tokenizer_test\")\n",
    "\n",
    "def test_tokenizer(tokenizer_path=None, sample_texts=None):\n",
    "    \"\"\"Test if the tokenizer works correctly.\"\"\"\n",
    "    try:\n",
    "        # If you're using a custom tokenizer\n",
    "        if tokenizer_path and os.path.exists(tokenizer_path):\n",
    "            logger.info(f\"Loading tokenizer from {tokenizer_path}\")\n",
    "            tokenizer = SimpleTokenizer.from_pretrained(tokenizer_path)\n",
    "        else:\n",
    "            # Fallback to standard GPT-2 tokenizer for comparison\n",
    "            logger.info(\"Loading standard GPT-2 tokenizer\")\n",
    "            tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        \n",
    "        # Set padding token - THIS IS THE FIX\n",
    "        logger.info(\"Setting pad_token to be the same as eos_token\")\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Test with sample texts\n",
    "        if not sample_texts:\n",
    "            sample_texts = [\n",
    "                \"This is a simple test sentence.\",\n",
    "                \"Let's test some domain-specific content that your model might see.\"\n",
    "            ]\n",
    "        \n",
    "        logger.info(\"Testing tokenization on sample texts:\")\n",
    "        for text in sample_texts:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            token_ids = tokenizer.encode(text)\n",
    "            decoded = tokenizer.decode(token_ids)\n",
    "            \n",
    "            logger.info(f\"\\nOriginal: {text}\")\n",
    "            logger.info(f\"Tokenized: {tokens}\")\n",
    "            logger.info(f\"Token IDs: {token_ids}\")\n",
    "            logger.info(f\"Decoded: {decoded}\")\n",
    "            logger.info(f\"Roundtrip successful: {text == decoded}\")\n",
    "        \n",
    "        # Test batch encoding\n",
    "        logger.info(\"\\nTesting batch encoding:\")\n",
    "        batch_encoding = tokenizer(sample_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logger.info(f\"Batch shape: {batch_encoding['input_ids'].shape}\")\n",
    "        \n",
    "        return True, \"Tokenizer test completed successfully\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Tokenizer test failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "# Execute the test\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your custom tokenizer if available\n",
    "    TOKENIZER_PATH = \"models/simple_taxa_tokenizer\"  # Set to None to use standard GPT-2\n",
    "    \n",
    "    # Sample texts from your domain\n",
    "    SAMPLE_TEXTS = [\n",
    "        # \"This is a simple test sentence.\",\n",
    "        # \"Let's test some domain-specific content that your model might see.\",\n",
    "        \"Roseburia Ruminococcus Streptococcus Dorea Faecalibacterium Anaerostipes Bifidobacterium Blautia Anaerobutyricum Agathobaculum Collinsella Klebsiella Fusicatenibacter Bacteroides Eubacterium Gemmiger Adlercreutzia Phocaeicola Alistipes Barnesiella Firmicutes\",\n",
    "        \"Ruminococcus Phocaeicola Bacteroides Faecalibacterium Eubacterium Roseburia Alistipes\"\n",
    "    ]\n",
    "    \n",
    "    success, message = test_tokenizer(TOKENIZER_PATH, SAMPLE_TEXTS)\n",
    "    logger.info(f\"Test result: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb0732c-17b5-45c1-8c8f-b64dc38f2856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c10bf714-0b7f-48a3-8f17-3882032730de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: Test data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29e3786-a583-4d57-a447-de24ac8aa818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import GPT2TokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"data_test\")\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "def test_data_loading(data_path, tokenizer_path, batch_size=4, max_samples=10):\n",
    "    \"\"\"Test if the data can be loaded and processed correctly.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading tokenizer from {tokenizer_path}\")\n",
    "        tokenizer = SimpleTokenizer.from_pretrained(tokenizer_path)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        logger.info(f\"Loading data from {data_path}\")\n",
    "        # Modify this part based on your data format\n",
    "        if data_path.endswith('.csv'):\n",
    "            df = pd.read_csv(data_path)\n",
    "            text_column = \"text\"  # Update this to your column name\n",
    "            texts = df[text_column].tolist()[:max_samples]\n",
    "        elif data_path.endswith('.txt'):\n",
    "            with open(data_path, 'r') as f:\n",
    "                texts = [line.strip() for line in f.readlines()[:max_samples]]\n",
    "        else:\n",
    "            # Add other data formats as needed\n",
    "            raise ValueError(f\"Unsupported data format: {data_path}\")\n",
    "            \n",
    "        logger.info(f\"Loaded {len(texts)} text samples\")\n",
    "        logger.info(f\"Sample text: {texts[0][:100]}...\")\n",
    "        \n",
    "        # Create dataset\n",
    "        logger.info(\"Creating dataset\")\n",
    "        dataset = SequenceDataset(texts, tokenizer)\n",
    "        \n",
    "        # Create dataloader\n",
    "        logger.info(\"Creating dataloader\")\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Test batch iteration\n",
    "        logger.info(\"Testing batch iteration\")\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            logger.info(f\"Batch {i+1} shape: {batch['input_ids'].shape}\")\n",
    "            if i >= 2:  # Just test a few batches\n",
    "                break\n",
    "                \n",
    "        logger.info(\"Data loading and processing test completed successfully\")\n",
    "        return True, \"Data test completed successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data test failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "# # Execute the test\n",
    "# if __name__ == \"__main__\":\n",
    "#     DATA_PATH = \"../../data/taxa_sequences.txt\"  # Update this\n",
    "#     TOKENIZER_PATH = \"gpt2\"  # Update this\n",
    "    \n",
    "#     success, message = test_data_loading(DATA_PATH, TOKENIZER_PATH)\n",
    "#     logger.info(f\"Test result: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f426540-b5de-46cd-b3bb-4d6ced1802dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf6ecb65-de09-4224-a9c3-3d794bbfcc93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# step 4: test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46aa24d-edc9-4543-bfa5-07ab8a10ba41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, SimpleTokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"model_test\")\n",
    "\n",
    "# In your model test\n",
    "def test_model(tokenizer_path, config=None, sample_text=\"This is a test\"):\n",
    "    try:\n",
    "        logger.info(f\"Loading tokenizer from {tokenizer_path}\")\n",
    "        tokenizer = SimpleTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        # Add this line:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Create or load model configuration\n",
    "        if config is None:\n",
    "            logger.info(\"Using default GPT-2 configuration\")\n",
    "            config = GPT2Config(\n",
    "                vocab_size=len(tokenizer),\n",
    "                n_positions=512,\n",
    "                n_ctx=512,\n",
    "                n_embd=768,\n",
    "                n_layer=6,\n",
    "                n_head=12,\n",
    "            )\n",
    "        \n",
    "        # Initialize model\n",
    "        logger.info(\"Initializing model\")\n",
    "        model = GPT2LMHeadModel(config)\n",
    "        \n",
    "        # Log model size\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        logger.info(f\"Model initialized with {total_params:,} parameters\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        logger.info(\"Testing forward pass\")\n",
    "        inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get available device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        \n",
    "        # Move model and inputs to device\n",
    "        model = model.to(device)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Add labels for loss calculation\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        # Forward pass with loss calculation\n",
    "        logger.info(\"Performing forward pass with loss calculation\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        logger.info(f\"Forward pass successful, loss: {outputs.loss.item()}\")\n",
    "        \n",
    "        # Test generation\n",
    "        logger.info(\"Testing text generation\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_length=50,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        logger.info(f\"Generated text: {generated_text}\")\n",
    "        \n",
    "        return True, \"Model test completed successfully\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model test failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "# Execute the test\n",
    "# if __name__ == \"__main__\":\n",
    "    # TOKENIZER_PATH = \"gpt2\"  # Update this\n",
    "    \n",
    "    # # Custom config if needed\n",
    "    # custom_config = GPT2Config(\n",
    "    #     vocab_size=50257,  # Update with your tokenizer's vocab size\n",
    "    #     n_positions=512,\n",
    "    #     n_ctx=512,\n",
    "    #     n_embd=768,\n",
    "    #     n_layer=6,  # Smaller than standard GPT-2 for faster testing\n",
    "    #     n_head=12,\n",
    "    # )\n",
    "    \n",
    "    # success, message = test_model(TOKENIZER_PATH, config=custom_config)\n",
    "    # logger.info(f\"Test result: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf43fee6-52d0-4031-b2dd-4955de763025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "148715a7-b225-485b-97cb-989cb4e7ee7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 5: Test minimal training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d491fba-6131-4e21-b9ed-7fc5434a9f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"training_test\")\n",
    "\n",
    "class MinimalDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "def test_training(tokenizer_path, num_steps=3):\n",
    "    \"\"\"Test a minimal training loop.\"\"\"\n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        logger.info(f\"Loading tokenizer from {tokenizer_path}\")\n",
    "        tokenizer = SimpleTokenizer.from_pretrained(tokenizer_path)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Create sample data\n",
    "        sample_texts = [\n",
    "            \"This is a test sentence for training GPT-2.\",\n",
    "            \"Let's see if we can train the model without errors.\",\n",
    "            \"Databricks runtime should be able to handle this small test.\",\n",
    "            \"If this works, we can move on to actual training.\"\n",
    "        ]\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        logger.info(\"Creating sample dataset\")\n",
    "        dataset = MinimalDataset(sample_texts, tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        logger.info(\"Initializing model\")\n",
    "        config = GPT2Config(\n",
    "            vocab_size=len(tokenizer),\n",
    "            n_positions=512,\n",
    "            n_ctx=512,\n",
    "            n_embd=768,\n",
    "            n_layer=4,  # Small model for testing\n",
    "            n_head=12,\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "        \n",
    "        # Get device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        logger.info(\"Setting up optimizer\")\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "        \n",
    "        # Training loop\n",
    "        logger.info(\"Starting mini training loop\")\n",
    "        model.train()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            batch = next(iter(dataloader))\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logger.info(f\"Step {step+1}/{num_steps}, Loss: {loss.item()}\")\n",
    "        \n",
    "        logger.info(\"Mini training loop completed successfully\")\n",
    "        return True, \"Training test completed successfully\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training test failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "# Execute the test\n",
    "# if __name__ == \"__main__\":\n",
    "#     TOKENIZER_PATH = \"gpt2\"  # Update this\n",
    "    \n",
    "#     success, message = test_training(TOKENIZER_PATH)\n",
    "#     logger.info(f\"Test result: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28925413-97b8-4b26-87ec-a9854b0ea5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90cfab9-f67c-4dfb-ab77-84e360b75eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 6: Test Databricks Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cecf2efb-9a9c-47a6-bce3-f5e0e1b5f061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"runtime_test\")\n",
    "\n",
    "def test_runtime():\n",
    "    \"\"\"Test Databricks runtime environment.\"\"\"\n",
    "    try:\n",
    "        # Python version\n",
    "        logger.info(f\"Python version: {sys.version}\")\n",
    "        \n",
    "        # PyTorch version and CUDA availability\n",
    "        logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "        logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "            logger.info(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                logger.info(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "                \n",
    "        # Transformers version\n",
    "        logger.info(f\"Transformers version: {transformers_version}\")\n",
    "        \n",
    "        # Memory info\n",
    "        memory = psutil.virtual_memory()\n",
    "        logger.info(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
    "        logger.info(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # CPU info\n",
    "        logger.info(f\"CPU count: {psutil.cpu_count()}\")\n",
    "        \n",
    "        # Disk space\n",
    "        disk = psutil.disk_usage('/')\n",
    "        logger.info(f\"Disk total: {disk.total / (1024**3):.2f} GB\")\n",
    "        logger.info(f\"Disk free: {disk.free / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Environment variables\n",
    "        logger.info(\"Relevant environment variables:\")\n",
    "        for var in ['CUDA_VISIBLE_DEVICES', 'PYTHONPATH', 'LD_LIBRARY_PATH']:\n",
    "            if var in os.environ:\n",
    "                logger.info(f\"{var}: {os.environ[var]}\")\n",
    "        \n",
    "        return True, \"Runtime test completed successfully\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Runtime test failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "# Execute the test\n",
    "if __name__ == \"__main__\":\n",
    "    success, message = test_runtime()\n",
    "    logger.info(f\"Test result: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961a68c8-9f63-46d5-9e51-3e15c891cf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83691f5a-ce30-4750-8472-42bdc7845157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae2398a-dc2b-42c9-84c4-52d08d73ad56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    GPT2Config, \n",
    "    GPT2LMHeadModel, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from utils_gpt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b20003-08f7-4d3c-abfd-d5a08e638a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Force CUDA initialization at the start\n",
    "if torch.cuda.is_available():\n",
    "    torch.zeros(1).cuda()  # Triggers CUDA context init\n",
    "    torch.cuda.manual_seed_all(42)  # Now safe to set seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb8ad20-a756-45d3-9d47-2d64a3483ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# customized tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099c8660-fd83-4603-93d8-475eddf76a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, token_to_id, id_to_token):\n",
    "        self.token_to_id = token_to_id\n",
    "        self.id_to_token = id_to_token\n",
    "        self.pad_token_id = token_to_id[\"<pad>\"]\n",
    "        self.bos_token_id = token_to_id[\"<s>\"]\n",
    "        self.eos_token_id = token_to_id[\"</s>\"]\n",
    "        self.unk_token_id = token_to_id[\"<unk>\"]\n",
    "        self.vocab_size = len(token_to_id)\n",
    "        \n",
    "        # Special token attributes expected by HF transformers\n",
    "        self.all_special_ids = [self.pad_token_id, self.bos_token_id, self.eos_token_id, self.unk_token_id]\n",
    "        self.model_max_length = MAX_SEQ_LENGTH\n",
    "\n",
    "        # Special token properties that HF expects\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.bos_token = \"<s>\"\n",
    "        self.eos_token = \"</s>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        \n",
    "        # Map IDs to special tokens\n",
    "        self.special_ids_to_tokens = {\n",
    "            self.pad_token_id: self.pad_token,\n",
    "            self.bos_token_id: self.bos_token,\n",
    "            self.eos_token_id: self.eos_token,\n",
    "            self.unk_token_id: self.unk_token\n",
    "        }\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Split text into tokens\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            return text.split()\n",
    "        return text\n",
    "\n",
    "    def __call__(self, sequence, max_length=None, padding=False, truncation=False, return_tensors=None):\n",
    "        \"\"\"Make the tokenizer callable like HF tokenizers\"\"\"\n",
    "        return self.encode(sequence, max_length, padding, truncation, return_tensors)\n",
    "    \n",
    "\n",
    "    def encode(self, sequence, max_length=None, padding=False, truncation=False, return_tensors=None):\n",
    "        \"\"\"Convert a sequence or batch of sequences to token IDs\"\"\"\n",
    "        # Check if it's a batch (list of strings)\n",
    "        if isinstance(sequence, list) and all(isinstance(s, str) for s in sequence):\n",
    "            # Process batch\n",
    "            batch_ids = []\n",
    "            batch_attention_masks = []\n",
    "            \n",
    "            # First pass: tokenize and truncate\n",
    "            for seq in sequence:\n",
    "                seq_tokens = seq.split()\n",
    "                \n",
    "                # Convert tokens to ids\n",
    "                ids = [self.token_to_id.get(token, self.unk_token_id) for token in seq_tokens]\n",
    "                \n",
    "                # Apply truncation if needed\n",
    "                if truncation and max_length and len(ids) > max_length:\n",
    "                    ids = ids[:max_length]\n",
    "                \n",
    "                batch_ids.append(ids)\n",
    "            \n",
    "            # Determine the padding length\n",
    "            if padding:\n",
    "                if max_length is None:\n",
    "                    # Pad to the longest sequence in the batch\n",
    "                    max_length = max(len(ids) for ids in batch_ids)\n",
    "                \n",
    "                # Second pass: pad all sequences to max_length\n",
    "                for i, ids in enumerate(batch_ids):\n",
    "                    original_length = len(ids)\n",
    "                    attention_mask = [1] * original_length\n",
    "                    \n",
    "                    padding_length = max_length - original_length\n",
    "                    if padding_length > 0:\n",
    "                        ids = ids + [self.pad_token_id] * padding_length\n",
    "                        attention_mask = attention_mask + [0] * padding_length\n",
    "                    \n",
    "                    batch_ids[i] = ids\n",
    "                    batch_attention_masks.append(attention_mask)\n",
    "            else:\n",
    "                # No padding - just create attention masks\n",
    "                for ids in batch_ids:\n",
    "                    attention_mask = [1] * len(ids)\n",
    "                    batch_attention_masks.append(attention_mask)\n",
    "            \n",
    "            # Return tensors if requested\n",
    "            if return_tensors == \"pt\":\n",
    "                return {\n",
    "                    \"input_ids\": torch.tensor(batch_ids),\n",
    "                    \"attention_mask\": torch.tensor(batch_attention_masks)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"input_ids\": batch_ids,\n",
    "                    \"attention_mask\": batch_attention_masks\n",
    "                }\n",
    "        else:\n",
    "            # Single sequence processing (original code)\n",
    "            if isinstance(sequence, str):\n",
    "                sequence = sequence.split()\n",
    "                \n",
    "            # Convert tokens to ids\n",
    "            ids = [self.token_to_id.get(token, self.unk_token_id) for token in sequence]\n",
    "            \n",
    "            # Apply truncation if needed\n",
    "            if truncation and max_length and len(ids) > max_length:\n",
    "                ids = ids[:max_length]\n",
    "                \n",
    "            # Apply padding if needed\n",
    "            attention_mask = [1] * len(ids)\n",
    "            if padding and max_length:\n",
    "                padding_length = max_length - len(ids)\n",
    "                ids = ids + [self.pad_token_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "            \n",
    "            # Return tensors if requested\n",
    "            if return_tensors == \"pt\":\n",
    "                return {\n",
    "                    \"input_ids\": torch.tensor([ids]),\n",
    "                    \"attention_mask\": torch.tensor([attention_mask])\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"input_ids\": ids,\n",
    "                    \"attention_mask\": attention_mask\n",
    "                }\n",
    "    \n",
    "    def decode(self, ids, skip_special_tokens=False):\n",
    "        \"\"\"Convert token IDs back to a sequence\"\"\"\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "            \n",
    "        tokens = []\n",
    "        for id in ids:\n",
    "            if skip_special_tokens and id in self.all_special_ids:\n",
    "                continue\n",
    "            tokens.append(self.id_to_token.get(id, \"<unk>\"))\n",
    "            \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def save_pretrained(self, output_dir):\n",
    "        \"\"\"Save tokenizer to disk\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the vocabulary\n",
    "        with open(os.path.join(output_dir, \"vocab.json\"), \"w\") as f:\n",
    "            # Convert int keys to strings for JSON serialization\n",
    "            token_to_id_serializable = {k: v for k, v in self.token_to_id.items()}\n",
    "            id_to_token_serializable = {str(k): v for k, v in self.id_to_token.items()}\n",
    "            json.dump({\n",
    "                \"token_to_id\": token_to_id_serializable,\n",
    "                \"id_to_token\": id_to_token_serializable\n",
    "            }, f)\n",
    "            \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, input_dir):\n",
    "        \"\"\"Load tokenizer from disk\"\"\"\n",
    "        with open(os.path.join(input_dir, \"vocab.json\"), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            token_to_id = data[\"token_to_id\"]\n",
    "            # Convert string keys back to integers\n",
    "            id_to_token = {int(k): v for k, v in data[\"id_to_token\"].items()}\n",
    "            \n",
    "        return cls(token_to_id, id_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4811f73-3f07-4919-ad4b-1bb2ad01f9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 150\n",
    "tokenizer_path = \"models/simple_taxa_tokenizer\"\n",
    "\n",
    "# load mapping dictionaries\n",
    "import json\n",
    "with open('../../data/token_to_id.json', 'r') as f:\n",
    "    token_to_id = json.load(f)\n",
    "id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = {\"<pad>\": 0, \"<s>\":  1, \n",
    "                    \"</s>\":  2, \"<unk>\": 3}\n",
    "token_to_id.update(special_tokens)\n",
    "id_to_token.update({v: k for k, v in special_tokens.items()})\n",
    "\n",
    "tokenizer = SimpleTokenizer(token_to_id, id_to_token)\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b8f2905-0f89-428b-9a3c-722ecc17f1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_TEXTS = [\n",
    "        # \"This is a simple test sentence.\",\n",
    "        # \"Let's test some domain-specific content that your model might see.\",\n",
    "        \"Roseburia Ruminococcus Streptococcus Dorea Faecalibacterium Anaerostipes Bifidobacterium Blautia Anaerobutyricum Agathobaculum Collinsella Klebsiella Fusicatenibacter Bacteroides Eubacterium Gemmiger Adlercreutzia Phocaeicola Alistipes Barnesiella Firmicutes\",\n",
    "        \"Ruminococcus Phocaeicola Bacteroides Faecalibacterium Eubacterium Roseburia Alistipes\"\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cb40f4-caa7-4563-860d-7178bd42e2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(SAMPLE_TEXTS, padding=True, truncation=True,\n",
    "                  return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a072785-2f1a-40e1-9cc7-c51c3d4d60c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# custom dataset class\n",
    "class TaxaSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: List of sequences, where each sequence is a list of string tokens\n",
    "            tokenizer: Simple tokenizer instance\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.inputs = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            encoded = tokenizer.encode(\n",
    "                seq,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            self.inputs.append(encoded)    \n",
    "\n",
    "            # self.inputs.append({\n",
    "            #     \"input_ids\": encoded[\"input_ids\"],\n",
    "            #     \"attention_mask\": encoded[\"attention_mask\"]\n",
    "            # })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get raw tensors\n",
    "        input_ids = self.inputs[idx][\"input_ids\"]\n",
    "        attention_mask = self.inputs[idx][\"attention_mask\"]\n",
    "        \n",
    "        # Ensure both tensors are 1D (flatten if needed)\n",
    "        if input_ids.dim() > 1:\n",
    "            input_ids = input_ids.view(-1)\n",
    "        if attention_mask.dim() > 1:\n",
    "            attention_mask = attention_mask.view(-1)\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31a3fae-cd3b-4835-9d35-59519b86ae71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# configure and initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f39bf7-bdcc-4197-9d5b-3c35c26e6c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "model chosen: GPT-2 Small (gpt2) \n",
    "\n",
    "- It's autoregressive, suited for generation task (taxa completion)\n",
    "- practical to extract embeddings for downstream supervised learning tasks\n",
    "- It handles variable sequence lengths well\n",
    "- With 355 unique values, no worry for vocabulary size issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87770240-ef55-4742-b8c4-d241753a2b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f7e3cf-bd0a-4ae9-8311-6f99a99ff2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_gpt2_model(vocab_size):\n",
    "    \"\"\"Create a GPT-2 model with custom vocab size\"\"\"\n",
    "    config = GPT2Config(\n",
    "        vocab_size=vocab_size,\n",
    "        n_positions=MAX_SEQ_LENGTH,\n",
    "        n_ctx=MAX_SEQ_LENGTH,\n",
    "        n_embd=64,  # Smaller embedding size\n",
    "        n_layer=6,   # Fewer layers for faster training\n",
    "        n_head=8,    # Fewer attention heads\n",
    "        bos_token_id=1,  # <s>\n",
    "        eos_token_id=2,  # </s>\n",
    "        pad_token_id=0,   # <pad>\n",
    "        attn_pdrop = 0.0,  # Attention dropout\n",
    "        embd_pdrop = 0.0,  # Embedding dropout\n",
    "        resid_pdrop = 0.0  # Residual dropout\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel(config)\n",
    "    return model\n",
    "\n",
    "model = create_gpt2_model(tokenizer.vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c332efd-bb07-45e8-a4aa-4b83466dbd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed33a34f-cfd1-4384-8ae6-cdff9f92874c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a SimpleDataCollator class to replace the HF DataCollatorForLanguageModeling, to Properly handles padding and creates proper language modeling labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eff66cd-1e8a-4d6b-bba2-a99e7426fa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimpleDataCollator:\n",
    "    \"\"\"Simple data collator for language modeling\"\"\"\n",
    "    def __init__(self, tokenizer, mlm=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        # Ensure consistent tensor shapes\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
    "        \n",
    "        # Get max length\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        \n",
    "        # Pad all tensors to max length\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            # Padding needed\n",
    "            padding_len = max_len - len(ids)\n",
    "            \n",
    "            if padding_len > 0:\n",
    "                # Pad with pad_token_id\n",
    "                padded_ids = torch.cat([\n",
    "                    ids, \n",
    "                    torch.full((padding_len,), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "                ])\n",
    "                padded_mask = torch.cat([\n",
    "                    mask,\n",
    "                    torch.zeros(padding_len, dtype=torch.long)\n",
    "                ])\n",
    "            else:\n",
    "                padded_ids = ids\n",
    "                padded_mask = mask\n",
    "                \n",
    "            padded_input_ids.append(padded_ids)\n",
    "            padded_attention_mask.append(padded_mask)\n",
    "        \n",
    "        # Stack into batches\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack(padded_input_ids),\n",
    "            \"attention_mask\": torch.stack(padded_attention_mask)\n",
    "        }\n",
    "        \n",
    "        # For causal language modeling\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        # Mark padding as -100 to ignore in loss calculation\n",
    "        labels[batch[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        def debug_batch(batch):\n",
    "            print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "            print(\"Attention mask shape:\", batch[\"attention_mask\"].shape)\n",
    "            print(\"Labels shape:\", batch[\"labels\"].shape)\n",
    "            # Check if all tensors are on the same device\n",
    "            print(\"Input IDs device:\", batch[\"input_ids\"].device)\n",
    "            print(\"Attention mask device:\", batch[\"attention_mask\"].device)\n",
    "            print(\"Labels device:\", batch[\"labels\"].device)\n",
    "            # Check for any NaN values\n",
    "            print(\"Any NaN in input_ids:\", torch.isnan(batch[\"input_ids\"]).any())\n",
    "            print(\"Any NaN in attention_mask:\", torch.isnan(batch[\"attention_mask\"]).any())\n",
    "            print(\"Any NaN in labels:\", torch.isnan(batch[\"labels\"]).any())\n",
    "            # Print some values\n",
    "            print(\"First sequence input_ids:\", batch[\"input_ids\"][0][:10])\n",
    "            print(\"First sequence attention_mask:\", batch[\"attention_mask\"][0][:10])\n",
    "            print(\"First sequence labels:\", batch[\"labels\"][0][:10])\n",
    "\n",
    "        # Use this before model forward pass\n",
    "        debug_batch(batch)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15bef3b3-1a43-488c-a11e-7497c98be1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520f9326-0d21-4198-a587-a0b0b5b11345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load sequences\n",
    "with open('../../data/taxa_sequences.txt', 'r') as file:\n",
    "    sequences = file.readlines()\n",
    "\n",
    "# Split sequences into train and test sets\n",
    "train_sequences, test_sequences = train_test_split(sequences, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11506ee0-7df6-4750-bc5d-188fcb46fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir=\"./gpt2_taxa_seq_model\"\n",
    "train_dataset = TaxaSequenceDataset(train_sequences, tokenizer)\n",
    "eval_dataset = TaxaSequenceDataset(test_sequences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a03e7ec-113e-468f-a604-402ba9b8cb02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_collator = SimpleDataCollator(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False\n",
    "    )\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        load_best_model_at_end=True if eval_dataset else False,\n",
    "        full_determinism = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8ef22f-adaf-4532-8a71-a46595a3fd1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"; os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5470251d-ed82-4452-b299-b9c492a898da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Training function (accept both training and evaluation datasets)\n",
    "def train_model(model, tokenizer, train_sequences, eval_sequences=None, output_dir=\"./gpt2_taxa_seq_model\"):\n",
    "    train_dataset = TaxaSequenceDataset(train_sequences, tokenizer)\n",
    "    \n",
    "    # Prepare validation dataset if provided\n",
    "    eval_dataset = None\n",
    "    if eval_sequences:\n",
    "        eval_dataset = TaxaSequenceDataset(eval_sequences, tokenizer)\n",
    "    \n",
    "    # Set up data collator with masked language modeling\n",
    "    data_collator = SimpleDataCollator(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        load_best_model_at_end=True if eval_dataset else False,\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the trained model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd77329f-3844-44c8-a6cd-1bb1a3ab38d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model, tokenizer = train_model(model, tokenizer, train_sequences, test_sequences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63cff74d-be12-429d-b685-e0ee38d759dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f41926-ebec-496c-ac8b-b75bac827c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# evaluate on sequence completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7adb7262-8f86-4696-969c-2ad909e01f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_sequence_completion(model, tokenizer, test_sequences, prefix_ratio=0.5):\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    \n",
    "    for sequence in test_sequences:\n",
    "        # Split the sequence into prefix and target\n",
    "        tokens = sequence.split()\n",
    "        prefix_len = max(3, int(len(tokens) * prefix_ratio))  # Use at least 3 tokens as prefix\n",
    "        \n",
    "        prefix = \" \".join(tokens[:prefix_len])\n",
    "        target = \" \".join(tokens[prefix_len:])\n",
    "        \n",
    "        # Generate completion using the prefix\n",
    "        generated = generate_from_seed(model, tokenizer, prefix, max_length=len(tokens) + 5)\n",
    "        \n",
    "        # Extract the generated continuation (after the prefix)\n",
    "        generated_completion = \" \".join(generated.split()[prefix_len:])\n",
    "        \n",
    "        # Calculate accuracy (exact match between tokens)\n",
    "        target_tokens = target.split()\n",
    "        generated_tokens = generated_completion.split()[:len(target_tokens)]  # Truncate to target length\n",
    "        \n",
    "        # If generated is shorter, pad with dummy values that won't match\n",
    "        if len(generated_tokens) < len(target_tokens):\n",
    "            generated_tokens.extend([\"DUMMY\"] * (len(target_tokens) - len(generated_tokens)))\n",
    "        \n",
    "        # Calculate token-level accuracy\n",
    "        matches = sum(1 for t, g in zip(target_tokens, generated_tokens) if t == g)\n",
    "        accuracy = matches / len(target_tokens) if target_tokens else 1.0\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return {\n",
    "        \"mean_accuracy\": np.mean(accuracies),\n",
    "        \"individual_accuracies\": accuracies\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b58fea9c-8f1e-46b8-9ee6-711f6c69c084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completion_results = evaluate_sequence_completion(model, tokenizer, test_sequences)\n",
    "print(f\"Mean sequence completion accuracy: {completion_results['mean_accuracy']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380e8c61-2f70-40ea-b8fd-f0a2464502b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example of sequence completion\n",
    "example_idx = np.random.randint(0, len(test_sequences))\n",
    "example_sequence = test_sequences[example_idx]\n",
    "tokens = example_sequence.split()\n",
    "prefix_len = max(3, int(len(tokens) * 0.5))\n",
    "prefix = \" \".join(tokens[:prefix_len])\n",
    "\n",
    "print(\"\\nExample sequence completion:\")\n",
    "print(f\"Prefix: {prefix}\")\n",
    "print(f\"Original completion: {' '.join(tokens[prefix_len:])}\")\n",
    "generated = generate_from_seed(model, tokenizer, prefix, max_length=len(tokens) + 5)\n",
    "print(f\"Model completion: {' '.join(generated.split()[prefix_len:])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e223e4f6-0b2f-4979-b5b1-107950956d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract embeddings for a few test sequences\n",
    "print(\"\\nExtracting embeddings for supervised learning...\")\n",
    "sample_sequences = test_sequences[:5]  # Just use a few sequences for demonstration\n",
    "embeddings = extract_embeddings(model, tokenizer, sample_sequences)\n",
    "print(f\"Extracted embeddings shape: {embeddings.shape}\")\n",
    "print(\"These embeddings can now be used for classification or regression tasks.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gpt2 trouble shooting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
