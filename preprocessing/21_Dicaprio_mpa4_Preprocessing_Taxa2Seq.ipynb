{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5746fce-d5dc-4935-afeb-03aa32125d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae2398a-dc2b-42c9-84c4-52d08d73ad56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a89ddc0-86d2-4928-82be-58af44af6678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29cdfec-e335-4bb7-975a-36d353d03371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**load the taxa abundance table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb5089d-3172-4577-8b9d-a0491652a884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_genus = spark.read.table(\"onesource_eu_dev_rni.onebiome.mpa4_genus_level\")\n",
    "df_genus =  df_genus.toPandas().set_index('barcode')\n",
    "print(df_genus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150ac73a-53ac-411c-9ee0-382c59d0b05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a3410b-9b42-4e03-866f-5dd5b454fa8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_genus.columns = df_genus.columns.str.lower()\n",
    "df_genus.columns = [col.split('|')[-1] for col in df_genus.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15e2a98-6eeb-4855-aa93-6583bedf3fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print([col for col in df_genus.columns if not col.startswith('g__')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1f8bf8-46a5-4473-9fa3-a11b237385d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_genus.columns = [col.split('|')[-1][3:] for col in df_genus.columns]\n",
    "df_genus.columns = df_genus.columns.str.lower()\n",
    "df_genus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a177c0-8220-4744-ae49-09346959fab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08c9778-dbcf-4189-be46-8b4eaf7d46b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similar_cols = []\n",
    "\n",
    "columns = df_genus.columns\n",
    "\n",
    "for i, col1 in enumerate(columns):\n",
    "    if len(col1)<1:\n",
    "        print(col1)\n",
    "        continue\n",
    "    for col2 in columns[i+1:]:\n",
    "        if len(col2)<1:\n",
    "            print(col2)\n",
    "            continue\n",
    "        if col1[:-1] == col2[:-1] and col1[-1] != col2[-1]:\n",
    "            similar_cols.extend([col1, col2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab360fca-12dc-4537-acc1-809a6bbcb4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similar_cols = sorted(set(similar_cols))\n",
    "import seaborn as sns\n",
    "df_genus_similar = df_genus[similar_cols]\n",
    "sns.heatmap(df_genus_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e57836-2127-46bb-a888-9f79556fa2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**generate the taxa sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49cc67e-5058-4a9c-b093-b53f100b6c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_sequences = []\n",
    "for _, row in df_genus.iterrows():\n",
    "    ranked_vars = row[row != 0].sort_values(ascending=False).index.tolist()  \n",
    "    list_sequences.append(ranked_vars) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca50147-5afe-4fb7-b2b0-c7cc5e953c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**check the sequence length distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5647fe1-9f5b-4f3a-b4fe-f8bcc0cdd756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = max([len(seq) for seq in list_sequences])\n",
    "print(MAX_SEQ_LENGTH)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(seq) for seq in list_sequences], bins=100)\n",
    "plt.title(\"Sequence Length Distribution in Dicaprio MPA4\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca607ce-3367-47a8-bd8c-0cc12a7c91a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sequences = [\" \".join(seq) for seq in list_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913589e5-dd66-4a2a-8573-eda56d0adfd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sequences[2222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72aa0e95-79b6-4ce2-8d9c-4c7b6f7bd11f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "976ed5ed-f40a-4dba-9178-5f6109e45672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/dicaprio_genus_sequences.txt', 'w') as file:\n",
    "    for sequence in sequences:\n",
    "        file.write(sequence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97580ede-b6b9-4acc-8995-a0aebabdd55b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extend the taxa id mapper \n",
    "to what was created based on curatedMetagenomics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69567a2-70d9-4997-8953-6654450fd179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UNIQUE_TAXA = df_genus.columns.sort_values()\n",
    "UNIQUE_TAXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0af2cab8-f577-41aa-b018-023c4b55c008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/genus_token_to_id_cmg.json', 'r') as file:\n",
    "    token_to_id = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b6735ab-d013-492e-b5db-8579bc484d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f158127d-0ab7-4155-81da-e152dcfcaf3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "token_to_id = {token: idx + 4 for idx, token in enumerate(UNIQUE_TAXA)}\n",
    "\n",
    "import json\n",
    "with open('../data/token_to_id.json', 'w') as file:\n",
    "    json.dump(token_to_id, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4811f73-3f07-4919-ad4b-1bb2ad01f9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = create_simple_mapping_tokenizer(sequences, UNIQUE_TAXA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cb40f4-caa7-4563-860d-7178bd42e2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(list_sequences[0], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc6480f-151d-41aa-b764-af40d3c1cfff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(list_sequences[0], return_tensors='pt')['input_ids'].dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a072785-2f1a-40e1-9cc7-c51c3d4d60c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# custom dataset class\n",
    "class TaxaSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: List of sequences, where each sequence is a list of string tokens\n",
    "            tokenizer: Simple tokenizer instance\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.inputs = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            encoded = tokenizer.encode(\n",
    "                seq,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            self.inputs.append(encoded)    \n",
    "\n",
    "            # self.inputs.append({\n",
    "            #     \"input_ids\": encoded[\"input_ids\"],\n",
    "            #     \"attention_mask\": encoded[\"attention_mask\"]\n",
    "            # })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get raw tensors\n",
    "        input_ids = self.inputs[idx][\"input_ids\"]\n",
    "        attention_mask = self.inputs[idx][\"attention_mask\"]\n",
    "        \n",
    "        # Ensure both tensors are 1D (flatten if needed)\n",
    "        if input_ids.dim() > 1:\n",
    "            input_ids = input_ids.view(-1)\n",
    "        if attention_mask.dim() > 1:\n",
    "            attention_mask = attention_mask.view(-1)\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31a3fae-cd3b-4835-9d35-59519b86ae71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# configure and initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f39bf7-bdcc-4197-9d5b-3c35c26e6c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "model chosen: GPT-2 Small (gpt2) \n",
    "\n",
    "- It's autoregressive, suited for generation task (taxa completion)\n",
    "- practical to extract embeddings for downstream supervised learning tasks\n",
    "- It handles variable sequence lengths well\n",
    "- With 355 unique values, no worry for vocabulary size issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f7e3cf-bd0a-4ae9-8311-6f99a99ff2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_gpt2_model(vocab_size):\n",
    "    \"\"\"Create a GPT-2 model with custom vocab size\"\"\"\n",
    "    config = GPT2Config(\n",
    "        vocab_size=vocab_size,\n",
    "        n_positions=MAX_SEQ_LENGTH,\n",
    "        n_ctx=MAX_SEQ_LENGTH,\n",
    "        n_embd=64,  # Smaller embedding size\n",
    "        n_layer=6,   # Fewer layers for faster training\n",
    "        n_head=8,    # Fewer attention heads\n",
    "        bos_token_id=vocab_size - 3,  # <s>\n",
    "        eos_token_id=vocab_size - 2,  # </s>\n",
    "        pad_token_id=vocab_size - 4,   # <pad>\n",
    "        attn_pdrop = 0.0,  # Attention dropout\n",
    "        embd_pdrop = 0.0,  # Embedding dropout\n",
    "        resid_pdrop = 0.0  # Residual dropout\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel(config)\n",
    "    return model\n",
    "\n",
    "model = create_gpt2_model(tokenizer.vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c332efd-bb07-45e8-a4aa-4b83466dbd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed33a34f-cfd1-4384-8ae6-cdff9f92874c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a SimpleDataCollator class to replace the HF DataCollatorForLanguageModeling, to Properly handles padding and creates proper language modeling labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eff66cd-1e8a-4d6b-bba2-a99e7426fa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimpleDataCollator:\n",
    "    \"\"\"Simple data collator for language modeling\"\"\"\n",
    "    def __init__(self, tokenizer, mlm=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        # Ensure consistent tensor shapes\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
    "        \n",
    "        # Get max length\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        \n",
    "        # Pad all tensors to max length\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            # Padding needed\n",
    "            padding_len = max_len - len(ids)\n",
    "            \n",
    "            if padding_len > 0:\n",
    "                # Pad with pad_token_id\n",
    "                padded_ids = torch.cat([\n",
    "                    ids, \n",
    "                    torch.full((padding_len,), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "                ])\n",
    "                padded_mask = torch.cat([\n",
    "                    mask,\n",
    "                    torch.zeros(padding_len, dtype=torch.long)\n",
    "                ])\n",
    "            else:\n",
    "                padded_ids = ids\n",
    "                padded_mask = mask\n",
    "                \n",
    "            padded_input_ids.append(padded_ids)\n",
    "            padded_attention_mask.append(padded_mask)\n",
    "        \n",
    "        # Stack into batches\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack(padded_input_ids),\n",
    "            \"attention_mask\": torch.stack(padded_attention_mask)\n",
    "        }\n",
    "        \n",
    "        # For causal language modeling\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        # Mark padding as -100 to ignore in loss calculation\n",
    "        labels[batch[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        def debug_batch(batch):\n",
    "            print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "            print(\"Attention mask shape:\", batch[\"attention_mask\"].shape)\n",
    "            print(\"Labels shape:\", batch[\"labels\"].shape)\n",
    "            # Check if all tensors are on the same device\n",
    "            print(\"Input IDs device:\", batch[\"input_ids\"].device)\n",
    "            print(\"Attention mask device:\", batch[\"attention_mask\"].device)\n",
    "            print(\"Labels device:\", batch[\"labels\"].device)\n",
    "            # Check for any NaN values\n",
    "            print(\"Any NaN in input_ids:\", torch.isnan(batch[\"input_ids\"]).any())\n",
    "            print(\"Any NaN in attention_mask:\", torch.isnan(batch[\"attention_mask\"]).any())\n",
    "            print(\"Any NaN in labels:\", torch.isnan(batch[\"labels\"]).any())\n",
    "            # Print some values\n",
    "            print(\"First sequence input_ids:\", batch[\"input_ids\"][0][:10])\n",
    "            print(\"First sequence attention_mask:\", batch[\"attention_mask\"][0][:10])\n",
    "            print(\"First sequence labels:\", batch[\"labels\"][0][:10])\n",
    "\n",
    "        # Use this before model forward pass\n",
    "        debug_batch(batch)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15bef3b3-1a43-488c-a11e-7497c98be1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7541ef-e091-42a2-b611-a902c3828d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11506ee0-7df6-4750-bc5d-188fcb46fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir=\"./gpt2_taxa_seq_model\"\n",
    "train_dataset = TaxaSequenceDataset(train_sequences, tokenizer)\n",
    "eval_dataset = TaxaSequenceDataset(test_sequences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a03e7ec-113e-468f-a604-402ba9b8cb02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_collator = SimpleDataCollator(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False\n",
    "    )\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        load_best_model_at_end=True if eval_dataset else False,\n",
    "        full_determinism = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8ef22f-adaf-4532-8a71-a46595a3fd1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"; os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5470251d-ed82-4452-b299-b9c492a898da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Training function (accept both training and evaluation datasets)\n",
    "def train_model(model, tokenizer, train_sequences, eval_sequences=None, output_dir=\"./gpt2_taxa_seq_model\"):\n",
    "    train_dataset = TaxaSequenceDataset(train_sequences, tokenizer)\n",
    "    \n",
    "    # Prepare validation dataset if provided\n",
    "    eval_dataset = None\n",
    "    if eval_sequences:\n",
    "        eval_dataset = TaxaSequenceDataset(eval_sequences, tokenizer)\n",
    "    \n",
    "    # Set up data collator with masked language modeling\n",
    "    data_collator = SimpleDataCollator(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        load_best_model_at_end=True if eval_dataset else False,\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the trained model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd77329f-3844-44c8-a6cd-1bb1a3ab38d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model, tokenizer = train_model(model, tokenizer, train_sequences, test_sequences)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f41926-ebec-496c-ac8b-b75bac827c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# evaluate on sequence completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7adb7262-8f86-4696-969c-2ad909e01f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_sequence_completion(model, tokenizer, test_sequences, prefix_ratio=0.5):\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    \n",
    "    for sequence in test_sequences:\n",
    "        # Split the sequence into prefix and target\n",
    "        tokens = sequence.split()\n",
    "        prefix_len = max(3, int(len(tokens) * prefix_ratio))  # Use at least 3 tokens as prefix\n",
    "        \n",
    "        prefix = \" \".join(tokens[:prefix_len])\n",
    "        target = \" \".join(tokens[prefix_len:])\n",
    "        \n",
    "        # Generate completion using the prefix\n",
    "        generated = generate_from_seed(model, tokenizer, prefix, max_length=len(tokens) + 5)\n",
    "        \n",
    "        # Extract the generated continuation (after the prefix)\n",
    "        generated_completion = \" \".join(generated.split()[prefix_len:])\n",
    "        \n",
    "        # Calculate accuracy (exact match between tokens)\n",
    "        target_tokens = target.split()\n",
    "        generated_tokens = generated_completion.split()[:len(target_tokens)]  # Truncate to target length\n",
    "        \n",
    "        # If generated is shorter, pad with dummy values that won't match\n",
    "        if len(generated_tokens) < len(target_tokens):\n",
    "            generated_tokens.extend([\"DUMMY\"] * (len(target_tokens) - len(generated_tokens)))\n",
    "        \n",
    "        # Calculate token-level accuracy\n",
    "        matches = sum(1 for t, g in zip(target_tokens, generated_tokens) if t == g)\n",
    "        accuracy = matches / len(target_tokens) if target_tokens else 1.0\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return {\n",
    "        \"mean_accuracy\": np.mean(accuracies),\n",
    "        \"individual_accuracies\": accuracies\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b58fea9c-8f1e-46b8-9ee6-711f6c69c084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completion_results = evaluate_sequence_completion(model, tokenizer, test_sequences)\n",
    "print(f\"Mean sequence completion accuracy: {completion_results['mean_accuracy']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380e8c61-2f70-40ea-b8fd-f0a2464502b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example of sequence completion\n",
    "example_idx = np.random.randint(0, len(test_sequences))\n",
    "example_sequence = test_sequences[example_idx]\n",
    "tokens = example_sequence.split()\n",
    "prefix_len = max(3, int(len(tokens) * 0.5))\n",
    "prefix = \" \".join(tokens[:prefix_len])\n",
    "\n",
    "print(\"\\nExample sequence completion:\")\n",
    "print(f\"Prefix: {prefix}\")\n",
    "print(f\"Original completion: {' '.join(tokens[prefix_len:])}\")\n",
    "generated = generate_from_seed(model, tokenizer, prefix, max_length=len(tokens) + 5)\n",
    "print(f\"Model completion: {' '.join(generated.split()[prefix_len:])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e223e4f6-0b2f-4979-b5b1-107950956d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract embeddings for a few test sequences\n",
    "print(\"\\nExtracting embeddings for supervised learning...\")\n",
    "sample_sequences = test_sequences[:5]  # Just use a few sequences for demonstration\n",
    "embeddings = extract_embeddings(model, tokenizer, sample_sequences)\n",
    "print(f\"Extracted embeddings shape: {embeddings.shape}\")\n",
    "print(\"These embeddings can now be used for classification or regression tasks.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "21_Dicaprio_mpa4_Preprocessing_Taxa2Seq",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
